{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Tal Waitzenberg 305578189 , Lital Morali 302491709"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./Data/AllTweets.csv', encoding = 'latin1')\n",
    "train = df[['text', 'screenName']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to clean the data from symbols, numbers ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n",
    "def clean_tweets(row_tweet):\n",
    "    # Function to convert a raw tweet to a string of words\n",
    "    # The input is a single string (a raw tweet), and \n",
    "    # the output is a single string (a preprocessed tweet)\n",
    "    \n",
    "    # 1. Remove twitter tags\n",
    "    row_tweet = re.sub('<.*?>', '', row_tweet)\n",
    "    \n",
    "    # 2. remove non-letters\n",
    "    row_tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', row_tweet, flags=re.MULTILINE)\n",
    "    row_tweet = re.sub(r'[^\\w\\s\\#\\@]', '', row_tweet)\n",
    "    row_tweet = re.sub(' +', ' ', row_tweet)\n",
    "    row_tweet = re.sub(r'\\d+', '', row_tweet)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    lower_case = row_tweet.lower()               # Convert to lower case \n",
    "    words = nltk.word_tokenize(lower_case)       # Split into words\n",
    "    words = lower_case.split()                   # Split by words, basic splitter without NLTK\n",
    "    \n",
    "    # 4. convert the stop words to a set\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    # 5.1. Reducing inflected and derived words to their word stem, base or root form\n",
    "    porter = nltk.PorterStemmer()\n",
    "    lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "    meaningful_words = [porter.stem(w) for w in words]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, and return the result.  \n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\talon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train['text'] = train['text'].apply(lambda x: clean_tweets(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepering the data for processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Split to training sets for each celebrity: Donald Trump, Kim Kardasihan, Katy Perry, Bill Gates, Kent Beck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_train = train[train['screenName'] == 'realDonaldTrump']\n",
    "kim_train = train[train['screenName'] == 'KimKardashian']\n",
    "katy_train = train[train['screenName'] == 'katyperry']\n",
    "bill_train = train[train['screenName'] == 'BillGates']\n",
    "kent_train = train[train['screenName'] == 'KentBeck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. create text data for each train of celebrity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Donald Trump Text To Train\n",
      "new rasmussen poll one accur elect trump approv ratingthat higher os #s. massiv regul cut new legisl bill sign great new scjustic infrastructur healthcar tax cut work. make america great agenda well despit distract witch hunt mani new job high busi enthusiasm. thought prayer sailor uss fitzgerald famili thank japanes alli th. rt @seanhann #hanniti start minut @newtgingrich monologu deep state alli media. back miami cubanamerican friend happi sign today anoth campaign promis forget. nation secur presidenti memorandum strengthen polici unit state toward cuba memorandum. remark presid trump polici usa toward cuba video transcript. great news #maga. investig fire fbi director man told fire fbi director witch hunt. despit phoni witch hunt go america econom amp job number great regul way job enthusiasm way. fake news media hate use turn power social media million peopl go around. month investig amp committe hear collus russian nobodi abl show proof sad. thank wisconsin tuesday great success #workforceweek @wctc w @ivankatrump amp @govwalk remark. crook h destroy phone w hammer bleach email amp husband meet wag day clear amp talk obstruct. hillari clinton famili dem deal russia look nondeal. wit singl greatest witch hunt american polit histori led bad conflict peopl #maga. made phoni collus russian stori found zero proof go obstruct justic phoni stori nice. left hospit rep steve scalis one truli great peopl tough shape real fighter pray steve. happi birthday us armi soldier thank braveri sacrific amp dedic proud commanderinchief. rep steve scalis louisiana true friend patriot badli injur fulli recov thought prayer. got back wisconsin great day great peopl. rt @tuckercarlson @richardgrenel @realdonaldtrump told tillerson full support us govt bring #ottowarmbi home. passag @deptvetaffair account whistleblow protect act great news veteran lo. arriv wisconsin discuss job job job #maga. million peopl drop obamacar death spiral obstructionist democrat gave answer resist. head great state wisconsin talk job job job big progress made real news report. fake news time high apolog incorrect stori. ag lynch made law enforc decis polit purposesgav hillari clinton free pass protect total illeg. rt @corrynmb @realdonaldtrump liber agenda america best interest keep fight good fight stand. well predict th circuit rule travel ban danger time histori countri sc. fake news media never wrong dirti purpos incorrect stori phoni sourc meet agenda hate sad. congratul @clemsonfb tiger full ceremoni #nationalchampion. final held first full @cabinet meet today great team restor american prosper br. congratul first new coal mine trump era open pennsylvania. never forget victim lost live one year ago today horrif #pulsenightclub shoot. ivanka @foxandfriend. great number economi work includ passag mani bill amp regul kill execut order kick. rt @foxandfriend @geraldorivera chanc impeach went comey testimoni. rt @foxandfriend yesterday hear provid zero evid collus campaign russian wasnt. daughter ivanka @foxandfriend tomorrow morn enjoy. democrat messag econom tax job fail #obamacar obstructionist. believ jame comey leak far preval anyon ever thought possibl total illeg cowardli. way regul way new job ad unemploy busi econom enthusiasm way record level. #fakenew msm doesnt report great econom news sinc elect day #dow #nasdaq drill amp energi sector. america go build budget ahead schedul time put #americafirst #infrastructureweek. time start build countri american worker amp american iron aluminum amp steel time. time rebuild countri bring back job restor dream amp ye put #americafirst ty. great honor host welcom leader around america @whitehous infrastructur summit. honor join @faithandfreedom coalit yesterday america dont worship govern worship god\n"
     ]
    }
   ],
   "source": [
    "trump_text_to_train = trump_train['text'].str.cat(sep='. ')\n",
    "kim_text_to_train = kim_train['text'].str.cat(sep='. ')\n",
    "katy_text_to_train = katy_train['text'].str.cat(sep='. ')\n",
    "bill_text_to_train = bill_train['text'].str.cat(sep='. ')\n",
    "kent_text_to_train = kent_train['text'].str.cat(sep='. ')\n",
    "print(\"Example: Donald Trump Text To Train\")\n",
    "print(trump_text_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing The Text\n",
    "a. create the vocabulary for each celebrity tweets\n",
    "    - W'll remove infrequent words. Words that appear one or two times will be removed from our vocabulary.\n",
    "      We want a small vocabulary to make our model to be not slow to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Donald Trump Vocabulary\n",
      "['agenda', 'alli', 'america', 'americafirst', 'american', 'amp', 'around', 'back', 'bill', 'bring', 'build', 'busi', 'campaign', 'clinton', 'collus', 'comey', 'congratul', 'countri', 'cuba', 'day', 'democrat', 'despit', 'econom', 'elect', 'enthusiasm', 'fake', 'famili', 'first', 'forget', 'foxandfriend', 'friend', 'full', 'go', 'great', 'happi', 'hate', 'hear', 'high', 'hillari', 'histori', 'honor', 'hunt', 'illeg', 'incorrect', 'infrastructur', 'investig', 'ivanka', 'job', 'made', 'maga', 'mani', 'media', 'meet', 'million', 'never', 'new', 'news', 'number', 'obamacar', 'obstruct', 'obstructionist', 'one', 'passag', 'peopl', 'phoni', 'polici', 'polit', 'prayer', 'proof', 'protect', 'put', 'real', 'realdonaldtrump', 'regul', 'remark', 'rep', 'report', 'restor', 'rt', 'russian', 'sad', 'scalis', 'sign', 'start', 'state', 'steve', 'stori', 'talk', 'tax', 'th', 'thank', 'thought', 'time', 'today', 'told', 'total', 'toward', 'trump', 'us', 'way', 'well', 'wisconsin', 'witch', 'work', 'yesterday', 'zero', 'TWEETSTART', 'TWEETEND']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_vocabulary(celebrity_train):\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000,\n",
    "                             min_df = 2) \n",
    "\n",
    "    vectorizer.fit_transform(celebrity_train)\n",
    "    return vectorizer.get_feature_names()\n",
    "\n",
    "trump_vocab = create_vocabulary(trump_train['text'])\n",
    "kim_vocab = create_vocabulary(kim_train['text'])\n",
    "katy_vocab = create_vocabulary(katy_train['text'])\n",
    "bill_vocab = create_vocabulary(bill_train['text'])\n",
    "kent_vocab = create_vocabulary(kent_train['text'])\n",
    "\n",
    "trump_vocab.append('TWEETSTART')\n",
    "trump_vocab.append('TWEETEND')\n",
    "\n",
    "\n",
    "print(\"Example: Donald Trump Vocabulary\")\n",
    "print(trump_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Convert special characters into strat token and end token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new rasmussen poll one accur elect trump approv ratingthat higher os #s TWEETEND TWEETSTART  massiv regul cut new legisl bill sign great new scjustic infrastructur healthcar tax cut work TWEETEND TWEETSTART  make america great agenda well despit distract witch hunt mani new job high busi enthusiasm TWEETEND TWEETSTART  thought prayer sailor uss fitzgerald famili thank japanes alli th TWEETEND TWEETSTART  rt @seanhann #hanniti start minut @newtgingrich monologu deep state alli media TWEETEND TWEETSTART  back miami cubanamerican friend happi sign today anoth campaign promis forget TWEETEND TWEETSTART  nation secur presidenti memorandum strengthen polici unit state toward cuba memorandum TWEETEND TWEETSTART  remark presid trump polici usa toward cuba video transcript TWEETEND TWEETSTART  great news #maga TWEETEND TWEETSTART  investig fire fbi director man told fire fbi director witch hunt TWEETEND TWEETSTART  despit phoni witch hunt go america econom amp job number great regul way job enthusiasm way TWEETEND TWEETSTART  fake news media hate use turn power social media million peopl go around TWEETEND TWEETSTART  month investig amp committe hear collus russian nobodi abl show proof sad TWEETEND TWEETSTART  thank wisconsin tuesday great success #workforceweek @wctc w @ivankatrump amp @govwalk remark TWEETEND TWEETSTART  crook h destroy phone w hammer bleach email amp husband meet wag day clear amp talk obstruct TWEETEND TWEETSTART  hillari clinton famili dem deal russia look nondeal TWEETEND TWEETSTART  wit singl greatest witch hunt american polit histori led bad conflict peopl #maga TWEETEND TWEETSTART  made phoni collus russian stori found zero proof go obstruct justic phoni stori nice TWEETEND TWEETSTART  left hospit rep steve scalis one truli great peopl tough shape real fighter pray steve TWEETEND TWEETSTART  happi birthday us armi soldier thank braveri sacrific amp dedic proud commanderinchief TWEETEND TWEETSTART  rep steve scalis louisiana true friend patriot badli injur fulli recov thought prayer TWEETEND TWEETSTART  got back wisconsin great day great peopl TWEETEND TWEETSTART  rt @tuckercarlson @richardgrenel @realdonaldtrump told tillerson full support us govt bring #ottowarmbi home TWEETEND TWEETSTART  passag @deptvetaffair account whistleblow protect act great news veteran lo TWEETEND TWEETSTART  arriv wisconsin discuss job job job #maga TWEETEND TWEETSTART  million peopl drop obamacar death spiral obstructionist democrat gave answer resist TWEETEND TWEETSTART  head great state wisconsin talk job job job big progress made real news report TWEETEND TWEETSTART  fake news time high apolog incorrect stori TWEETEND TWEETSTART  ag lynch made law enforc decis polit purposesgav hillari clinton free pass protect total illeg TWEETEND TWEETSTART  rt @corrynmb @realdonaldtrump liber agenda america best interest keep fight good fight stand TWEETEND TWEETSTART  well predict th circuit rule travel ban danger time histori countri sc TWEETEND TWEETSTART  fake news media never wrong dirti purpos incorrect stori phoni sourc meet agenda hate sad TWEETEND TWEETSTART  congratul @clemsonfb tiger full ceremoni #nationalchampion TWEETEND TWEETSTART  final held first full @cabinet meet today great team restor american prosper br TWEETEND TWEETSTART  congratul first new coal mine trump era open pennsylvania TWEETEND TWEETSTART  never forget victim lost live one year ago today horrif #pulsenightclub shoot TWEETEND TWEETSTART  ivanka @foxandfriend TWEETEND TWEETSTART  great number economi work includ passag mani bill amp regul kill execut order kick TWEETEND TWEETSTART  rt @foxandfriend @geraldorivera chanc impeach went comey testimoni TWEETEND TWEETSTART  rt @foxandfriend yesterday hear provid zero evid collus campaign russian wasnt TWEETEND TWEETSTART  daughter ivanka @foxandfriend tomorrow morn enjoy TWEETEND TWEETSTART  democrat messag econom tax job fail #obamacar obstructionist TWEETEND TWEETSTART  believ jame comey leak far preval anyon ever thought possibl total illeg cowardli TWEETEND TWEETSTART  way regul way new job ad unemploy busi econom enthusiasm way record level TWEETEND TWEETSTART  #fakenew msm doesnt report great econom news sinc elect day #dow #nasdaq drill amp energi sector TWEETEND TWEETSTART  america go build budget ahead schedul time put #americafirst #infrastructureweek TWEETEND TWEETSTART  time start build countri american worker amp american iron aluminum amp steel time TWEETEND TWEETSTART  time rebuild countri bring back job restor dream amp ye put #americafirst ty TWEETEND TWEETSTART  great honor host welcom leader around america @whitehous infrastructur summit TWEETEND TWEETSTART  honor join @faithandfreedom coalit yesterday america dont worship govern worship god'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_start_token = \"TWEETSTART\"\n",
    "tweet_end_token = \"TWEETEND\"\n",
    "\n",
    "def replace_special_chars(text):\n",
    "    return text.replace('.',' '+ tweet_end_token + ' ' + tweet_start_token+' ' )\n",
    "\n",
    "trump_text_to_train = replace_special_chars(trump_text_to_train)\n",
    "kim_text_to_train = replace_special_chars(kim_text_to_train)\n",
    "katy_text_to_train = replace_special_chars(katy_text_to_train)\n",
    "bill_text_to_train = replace_special_chars(bill_text_to_train)\n",
    "kent_text_to_train = replace_special_chars(kent_text_to_train)\n",
    "trump_text_to_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. We replace all word not included in our vocabularies by UNKNOWN_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new UNKNOWNTOKEN UNKNOWNTOKEN one UNKNOWNTOKEN elect trump UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  UNKNOWNTOKEN regul UNKNOWNTOKEN new UNKNOWNTOKEN bill sign great new UNKNOWNTOKEN infrastructur UNKNOWNTOKEN tax UNKNOWNTOKEN work TWEETEND TWEETSTART  UNKNOWNTOKEN america great agenda well despit UNKNOWNTOKEN witch hunt mani new job high busi enthusiasm TWEETEND TWEETSTART  thought prayer UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN famili thank UNKNOWNTOKEN alli th TWEETEND TWEETSTART  rt UNKNOWNTOKEN UNKNOWNTOKEN start UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN state alli media TWEETEND TWEETSTART  back UNKNOWNTOKEN UNKNOWNTOKEN friend happi sign today UNKNOWNTOKEN campaign UNKNOWNTOKEN forget TWEETEND TWEETSTART  UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN polici UNKNOWNTOKEN state toward cuba UNKNOWNTOKEN TWEETEND TWEETSTART  remark UNKNOWNTOKEN trump polici UNKNOWNTOKEN toward cuba UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  great news UNKNOWNTOKEN TWEETEND TWEETSTART  investig UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN told UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN witch hunt TWEETEND TWEETSTART  despit phoni witch hunt go america econom amp job number great regul way job enthusiasm way TWEETEND TWEETSTART  fake news media hate UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN media million peopl go around TWEETEND TWEETSTART  UNKNOWNTOKEN investig amp UNKNOWNTOKEN hear collus russian UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN proof sad TWEETEND TWEETSTART  thank wisconsin UNKNOWNTOKEN great UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN amp UNKNOWNTOKEN remark TWEETEND TWEETSTART  UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN amp UNKNOWNTOKEN meet UNKNOWNTOKEN day UNKNOWNTOKEN amp talk obstruct TWEETEND TWEETSTART  hillari clinton famili UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN witch hunt american polit histori UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN peopl UNKNOWNTOKEN TWEETEND TWEETSTART  made phoni collus russian stori UNKNOWNTOKEN zero proof go obstruct UNKNOWNTOKEN phoni stori UNKNOWNTOKEN TWEETEND TWEETSTART  UNKNOWNTOKEN UNKNOWNTOKEN rep steve scalis one UNKNOWNTOKEN great peopl UNKNOWNTOKEN UNKNOWNTOKEN real UNKNOWNTOKEN UNKNOWNTOKEN steve TWEETEND TWEETSTART  happi UNKNOWNTOKEN us UNKNOWNTOKEN UNKNOWNTOKEN thank UNKNOWNTOKEN UNKNOWNTOKEN amp UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  rep steve scalis UNKNOWNTOKEN UNKNOWNTOKEN friend UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN thought prayer TWEETEND TWEETSTART  UNKNOWNTOKEN back wisconsin great day great peopl TWEETEND TWEETSTART  rt UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN told UNKNOWNTOKEN full UNKNOWNTOKEN us UNKNOWNTOKEN bring UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  passag UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN protect UNKNOWNTOKEN great news UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  UNKNOWNTOKEN wisconsin UNKNOWNTOKEN job job job UNKNOWNTOKEN TWEETEND TWEETSTART  million peopl UNKNOWNTOKEN obamacar UNKNOWNTOKEN UNKNOWNTOKEN obstructionist democrat UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  UNKNOWNTOKEN great state wisconsin talk job job job UNKNOWNTOKEN UNKNOWNTOKEN made real news report TWEETEND TWEETSTART  fake news time high UNKNOWNTOKEN incorrect stori TWEETEND TWEETSTART  UNKNOWNTOKEN UNKNOWNTOKEN made UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN polit UNKNOWNTOKEN hillari clinton UNKNOWNTOKEN UNKNOWNTOKEN protect total illeg TWEETEND TWEETSTART  rt UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN agenda america UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  well UNKNOWNTOKEN th UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN time histori countri UNKNOWNTOKEN TWEETEND TWEETSTART  fake news media never UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN incorrect stori phoni UNKNOWNTOKEN meet agenda hate sad TWEETEND TWEETSTART  congratul UNKNOWNTOKEN UNKNOWNTOKEN full UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  UNKNOWNTOKEN UNKNOWNTOKEN first full UNKNOWNTOKEN meet today great UNKNOWNTOKEN restor american UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  congratul first new UNKNOWNTOKEN UNKNOWNTOKEN trump UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  never forget UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN one UNKNOWNTOKEN UNKNOWNTOKEN today UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  ivanka UNKNOWNTOKEN TWEETEND TWEETSTART  great number UNKNOWNTOKEN work UNKNOWNTOKEN passag mani bill amp regul UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  rt UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN comey UNKNOWNTOKEN TWEETEND TWEETSTART  rt UNKNOWNTOKEN yesterday hear UNKNOWNTOKEN zero UNKNOWNTOKEN collus campaign russian UNKNOWNTOKEN TWEETEND TWEETSTART  UNKNOWNTOKEN ivanka UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  democrat UNKNOWNTOKEN econom tax job UNKNOWNTOKEN UNKNOWNTOKEN obstructionist TWEETEND TWEETSTART  UNKNOWNTOKEN UNKNOWNTOKEN comey UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN thought UNKNOWNTOKEN total illeg UNKNOWNTOKEN TWEETEND TWEETSTART  way regul way new job UNKNOWNTOKEN UNKNOWNTOKEN busi econom enthusiasm way UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN report great econom news UNKNOWNTOKEN elect day UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN amp UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  america go build UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN time put UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  time start build countri american UNKNOWNTOKEN amp american UNKNOWNTOKEN UNKNOWNTOKEN amp UNKNOWNTOKEN time TWEETEND TWEETSTART  time UNKNOWNTOKEN countri bring back job restor UNKNOWNTOKEN amp UNKNOWNTOKEN put UNKNOWNTOKEN UNKNOWNTOKEN TWEETEND TWEETSTART  great honor UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN around america UNKNOWNTOKEN infrastructur UNKNOWNTOKEN TWEETEND TWEETSTART  honor UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN yesterday america UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN god'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_token = 'UNKNOWNTOKEN'   \n",
    "def replace_with_unknown_token(text,vocab):\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            text = text.replace(' ' + word + ' ', ' ' + unknown_token + ' ')\n",
    "    return text\n",
    "\n",
    "trump_text_to_train = replace_with_unknown_token(trump_text_to_train, trump_vocab)\n",
    "kim_text_to_train = replace_with_unknown_token(kim_text_to_train, kim_vocab)\n",
    "katy_text_to_train = replace_with_unknown_token(katy_text_to_train, katy_vocab)\n",
    "bill_text_to_train = replace_with_unknown_token(bill_text_to_train, bill_vocab)\n",
    "kent_text_to_train = replace_with_unknown_token(kent_text_to_train, kent_vocab)\n",
    "trump_text_to_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. We will use text_to_word_sequence to splits a tweet into a list of word for each celebrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'UNKNOWNTOKEN',\n",
       " 'UNKNOWNTOKEN',\n",
       " 'one',\n",
       " 'UNKNOWNTOKEN',\n",
       " 'elect',\n",
       " 'trump',\n",
       " 'UNKNOWNTOKEN',\n",
       " 'UNKNOWNTOKEN',\n",
       " 'UNKNOWNTOKEN']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "trump_seq = text_to_word_sequence(trump_text_to_train, lower=False, split=\" \")\n",
    "kim_seq = text_to_word_sequence(kim_text_to_train, lower=False, split=\" \")\n",
    "katy_seq = text_to_word_sequence(katy_text_to_train, lower=False, split=\" \")\n",
    "bill_seq = text_to_word_sequence(bill_text_to_train, lower=False, split=\" \")\n",
    "kent_seq = text_to_word_sequence(kent_text_to_train, lower=False, split=\" \")\n",
    "trump_seq[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. We will create a toknizer for each celebrity and train it to get the a matrix for each celebrity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenizer for vectorizing text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Donald Trump Matrix\n",
    "trump_token = Tokenizer(num_words=600,char_level=False)\n",
    "trump_token.fit_on_texts(trump_seq)\n",
    "trump_text_mtx = trump_token.texts_to_matrix(trump_seq, mode='binary')\n",
    "\n",
    "#Kim Kardashian Matrix\n",
    "kim_token = Tokenizer(num_words=600,char_level=False)\n",
    "kim_token.fit_on_texts(kim_seq)\n",
    "kim_text_mtx = kim_token.texts_to_matrix(kim_seq, mode='binary')\n",
    "\n",
    "#Katy Perry Matrix\n",
    "katy_token = Tokenizer(num_words=600,char_level=False)\n",
    "katy_token.fit_on_texts(katy_seq)\n",
    "katy_text_mtx = katy_token.texts_to_matrix(katy_seq, mode='binary')\n",
    "\n",
    "#Bill Gates Matrix\n",
    "bill_token = Tokenizer(num_words=600,char_level=False)\n",
    "bill_token.fit_on_texts(bill_seq)\n",
    "bill_text_mtx = bill_token.texts_to_matrix(bill_seq, mode='binary')\n",
    "\n",
    "#Kent Beck Matrix\n",
    "kent_token = Tokenizer(num_words=600,char_level=False)\n",
    "kent_token.fit_on_texts(kent_seq)\n",
    "kent_text_mtx = kent_token.texts_to_matrix(kent_seq, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input And Output<br />\n",
    "We will create input & output for each celebrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. We want to predict the next word, so output will be the input matrix shifted by one row. <br />\n",
    "b. checking that they both have the same number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((662, 600), (662, 600))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Donald Trump Input&Output\n",
    "trump_input = trump_text_mtx[:-1]\n",
    "trump_output = trump_text_mtx[1:]\n",
    "\n",
    "#Kim Kardashian Input&Output\n",
    "kim_input = kim_text_mtx[:-1]\n",
    "kim_output = kim_text_mtx[1:]\n",
    "\n",
    "#Katy Perry Input&Output\n",
    "katy_input = katy_text_mtx[:-1]\n",
    "katy_output = katy_text_mtx[1:]\n",
    "\n",
    "#Bill Gates Input&Output\n",
    "bill_input = bill_text_mtx[:-1]\n",
    "bill_output = bill_text_mtx[1:]\n",
    "\n",
    "#Kent Beck Input&Output\n",
    "kent_input = kent_text_mtx[:-1]\n",
    "kent_output = kent_text_mtx[1:]\n",
    "\n",
    "trump_input.shape, trump_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training the Model</h2><br />\n",
    "We will train 5 models for each celebrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Donald Trump Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 600, 42)           27804     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               15120600  \n",
      "=================================================================\n",
      "Total params: 15,148,404\n",
      "Trainable params: 15,148,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 529 samples, validate on 133 samples\n",
      "Epoch 1/100\n",
      "529/529 [==============================] - 1s - loss: 5.9202 - acc: 0.0000e+00 - val_loss: 4.8730 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "529/529 [==============================] - 1s - loss: 4.5352 - acc: 0.0095 - val_loss: 4.5676 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "529/529 [==============================] - 1s - loss: 4.2139 - acc: 0.0095 - val_loss: 4.1869 - val_acc: 0.4211\n",
      "Epoch 4/100\n",
      "529/529 [==============================] - 1s - loss: 3.9710 - acc: 0.3970 - val_loss: 4.0078 - val_acc: 0.4211\n",
      "Epoch 5/100\n",
      "529/529 [==============================] - 1s - loss: 3.7025 - acc: 0.3970 - val_loss: 3.8132 - val_acc: 0.4211\n",
      "Epoch 6/100\n",
      "529/529 [==============================] - 1s - loss: 3.5044 - acc: 0.3970 - val_loss: 3.7339 - val_acc: 0.4211\n",
      "Epoch 7/100\n",
      "529/529 [==============================] - 1s - loss: 3.4792 - acc: 0.3970 - val_loss: 3.6585 - val_acc: 0.4211\n",
      "Epoch 8/100\n",
      "529/529 [==============================] - 1s - loss: 3.4482 - acc: 0.3970 - val_loss: 3.6514 - val_acc: 0.4211\n",
      "Epoch 9/100\n",
      "529/529 [==============================] - 1s - loss: 3.3390 - acc: 0.3970 - val_loss: 3.6514 - val_acc: 0.4211\n",
      "Epoch 10/100\n",
      "529/529 [==============================] - 1s - loss: 3.3413 - acc: 0.3970 - val_loss: 3.5456 - val_acc: 0.4211\n",
      "Epoch 11/100\n",
      "529/529 [==============================] - 1s - loss: 3.2944 - acc: 0.3970 - val_loss: 3.4858 - val_acc: 0.4211\n",
      "Epoch 12/100\n",
      "529/529 [==============================] - 1s - loss: 3.2362 - acc: 0.3970 - val_loss: 3.5392 - val_acc: 0.4211\n",
      "Epoch 13/100\n",
      "529/529 [==============================] - 1s - loss: 3.2412 - acc: 0.3970 - val_loss: 3.4432 - val_acc: 0.4211\n",
      "Epoch 14/100\n",
      "529/529 [==============================] - 1s - loss: 3.2193 - acc: 0.3970 - val_loss: 3.4599 - val_acc: 0.4211\n",
      "Epoch 15/100\n",
      "529/529 [==============================] - 1s - loss: 3.2154 - acc: 0.3970 - val_loss: 3.4826 - val_acc: 0.4211\n",
      "Epoch 16/100\n",
      "529/529 [==============================] - 1s - loss: 3.2112 - acc: 0.3970 - val_loss: 3.4995 - val_acc: 0.4211\n",
      "Epoch 17/100\n",
      "529/529 [==============================] - 1s - loss: 3.1852 - acc: 0.3970 - val_loss: 3.5123 - val_acc: 0.4211\n",
      "Epoch 18/100\n",
      "529/529 [==============================] - 1s - loss: 3.2224 - acc: 0.3970 - val_loss: 3.4587 - val_acc: 0.4211\n",
      "Epoch 19/100\n",
      "529/529 [==============================] - 1s - loss: 3.1938 - acc: 0.3970 - val_loss: 3.4807 - val_acc: 0.4211\n",
      "Epoch 20/100\n",
      "529/529 [==============================] - 1s - loss: 3.1662 - acc: 0.3970 - val_loss: 3.5415 - val_acc: 0.4211\n",
      "Epoch 21/100\n",
      "529/529 [==============================] - 1s - loss: 3.1718 - acc: 0.3970 - val_loss: 3.5212 - val_acc: 0.4211\n",
      "Epoch 22/100\n",
      "529/529 [==============================] - 1s - loss: 3.1610 - acc: 0.3970 - val_loss: 3.5367 - val_acc: 0.4211\n",
      "Epoch 23/100\n",
      "529/529 [==============================] - 1s - loss: 3.1568 - acc: 0.3970 - val_loss: 3.5320 - val_acc: 0.4211\n",
      "Epoch 24/100\n",
      "529/529 [==============================] - 1s - loss: 3.1656 - acc: 0.3970 - val_loss: 3.5149 - val_acc: 0.4211\n",
      "Epoch 25/100\n",
      "529/529 [==============================] - 1s - loss: 3.1260 - acc: 0.3970 - val_loss: 3.5786 - val_acc: 0.4211\n",
      "Epoch 26/100\n",
      "529/529 [==============================] - 1s - loss: 3.1529 - acc: 0.3970 - val_loss: 3.4907 - val_acc: 0.4211\n",
      "Epoch 27/100\n",
      "529/529 [==============================] - 1s - loss: 3.1458 - acc: 0.3970 - val_loss: 3.5308 - val_acc: 0.4211\n",
      "Epoch 28/100\n",
      "529/529 [==============================] - 1s - loss: 3.1329 - acc: 0.3970 - val_loss: 3.5545 - val_acc: 0.4211\n",
      "Epoch 29/100\n",
      "529/529 [==============================] - 1s - loss: 3.1124 - acc: 0.3970 - val_loss: 3.5270 - val_acc: 0.4211\n",
      "Epoch 30/100\n",
      "529/529 [==============================] - 1s - loss: 3.1091 - acc: 0.3970 - val_loss: 3.5149 - val_acc: 0.4211\n",
      "Epoch 31/100\n",
      "529/529 [==============================] - 1s - loss: 3.0993 - acc: 0.3970 - val_loss: 3.5224 - val_acc: 0.4211\n",
      "Epoch 32/100\n",
      "529/529 [==============================] - 1s - loss: 3.1114 - acc: 0.3970 - val_loss: 3.4905 - val_acc: 0.4211\n",
      "Epoch 33/100\n",
      "529/529 [==============================] - 1s - loss: 3.0820 - acc: 0.3970 - val_loss: 3.5407 - val_acc: 0.4211\n",
      "Epoch 34/100\n",
      "529/529 [==============================] - 1s - loss: 3.0581 - acc: 0.3970 - val_loss: 3.5288 - val_acc: 0.4211\n",
      "Epoch 35/100\n",
      "529/529 [==============================] - 1s - loss: 3.0716 - acc: 0.3970 - val_loss: 3.5547 - val_acc: 0.4211\n",
      "Epoch 36/100\n",
      "529/529 [==============================] - 1s - loss: 3.0594 - acc: 0.3970 - val_loss: 3.5492 - val_acc: 0.4211\n",
      "Epoch 37/100\n",
      "529/529 [==============================] - 1s - loss: 3.0648 - acc: 0.3970 - val_loss: 3.5283 - val_acc: 0.4211\n",
      "Epoch 38/100\n",
      "529/529 [==============================] - 1s - loss: 3.0518 - acc: 0.3970 - val_loss: 3.5373 - val_acc: 0.4211\n",
      "Epoch 39/100\n",
      "529/529 [==============================] - 1s - loss: 3.0464 - acc: 0.3970 - val_loss: 3.5053 - val_acc: 0.4211\n",
      "Epoch 40/100\n",
      "529/529 [==============================] - 1s - loss: 3.0260 - acc: 0.3970 - val_loss: 3.5276 - val_acc: 0.4211\n",
      "Epoch 41/100\n",
      "529/529 [==============================] - 1s - loss: 3.0088 - acc: 0.3970 - val_loss: 3.5316 - val_acc: 0.4211\n",
      "Epoch 42/100\n",
      "529/529 [==============================] - 1s - loss: 3.0089 - acc: 0.3970 - val_loss: 3.5643 - val_acc: 0.4211\n",
      "Epoch 43/100\n",
      "529/529 [==============================] - 1s - loss: 3.0121 - acc: 0.3970 - val_loss: 3.5575 - val_acc: 0.4211\n",
      "Epoch 44/100\n",
      "529/529 [==============================] - 1s - loss: 2.9979 - acc: 0.3970 - val_loss: 3.5411 - val_acc: 0.4211\n",
      "Epoch 45/100\n",
      "529/529 [==============================] - 1s - loss: 2.9779 - acc: 0.3970 - val_loss: 3.5497 - val_acc: 0.4211\n",
      "Epoch 46/100\n",
      "529/529 [==============================] - 1s - loss: 2.9749 - acc: 0.3970 - val_loss: 3.5590 - val_acc: 0.4211\n",
      "Epoch 47/100\n",
      "529/529 [==============================] - 1s - loss: 2.9743 - acc: 0.3970 - val_loss: 3.5577 - val_acc: 0.4211\n",
      "Epoch 48/100\n",
      "529/529 [==============================] - 1s - loss: 2.9674 - acc: 0.3970 - val_loss: 3.5646 - val_acc: 0.4211\n",
      "Epoch 49/100\n",
      "529/529 [==============================] - 1s - loss: 2.9420 - acc: 0.3970 - val_loss: 3.5465 - val_acc: 0.4211\n",
      "Epoch 50/100\n",
      "529/529 [==============================] - 1s - loss: 2.9501 - acc: 0.3970 - val_loss: 3.5470 - val_acc: 0.4211\n",
      "Epoch 51/100\n",
      "529/529 [==============================] - 1s - loss: 2.9400 - acc: 0.3970 - val_loss: 3.5970 - val_acc: 0.4211\n",
      "Epoch 52/100\n",
      "529/529 [==============================] - 1s - loss: 2.9382 - acc: 0.3970 - val_loss: 3.5623 - val_acc: 0.4211\n",
      "Epoch 53/100\n",
      "529/529 [==============================] - 1s - loss: 2.8995 - acc: 0.3970 - val_loss: 3.5690 - val_acc: 0.4211\n",
      "Epoch 54/100\n",
      "529/529 [==============================] - 1s - loss: 2.8932 - acc: 0.3970 - val_loss: 3.5860 - val_acc: 0.4211\n",
      "Epoch 55/100\n",
      "529/529 [==============================] - 1s - loss: 2.9165 - acc: 0.3970 - val_loss: 3.6124 - val_acc: 0.4211\n",
      "Epoch 56/100\n",
      "529/529 [==============================] - 1s - loss: 2.8888 - acc: 0.3970 - val_loss: 3.5856 - val_acc: 0.4211\n",
      "Epoch 57/100\n",
      "529/529 [==============================] - 1s - loss: 2.8875 - acc: 0.3970 - val_loss: 3.5531 - val_acc: 0.4211\n",
      "Epoch 58/100\n",
      "529/529 [==============================] - 1s - loss: 2.8657 - acc: 0.3970 - val_loss: 3.5851 - val_acc: 0.4211\n",
      "Epoch 59/100\n",
      "529/529 [==============================] - 1s - loss: 2.8544 - acc: 0.3970 - val_loss: 3.5614 - val_acc: 0.4211\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529/529 [==============================] - 1s - loss: 2.8334 - acc: 0.3970 - val_loss: 3.5962 - val_acc: 0.4211\n",
      "Epoch 61/100\n",
      "529/529 [==============================] - 1s - loss: 2.8467 - acc: 0.3970 - val_loss: 3.5655 - val_acc: 0.4962\n",
      "Epoch 62/100\n",
      "529/529 [==============================] - 1s - loss: 2.8076 - acc: 0.4480 - val_loss: 3.5923 - val_acc: 0.4211\n",
      "Epoch 63/100\n",
      "529/529 [==============================] - 1s - loss: 2.7975 - acc: 0.3970 - val_loss: 3.5841 - val_acc: 0.4211\n",
      "Epoch 64/100\n",
      "529/529 [==============================] - 1s - loss: 2.7879 - acc: 0.3970 - val_loss: 3.5736 - val_acc: 0.4211\n",
      "Epoch 65/100\n",
      "529/529 [==============================] - 1s - loss: 2.7823 - acc: 0.3970 - val_loss: 3.5671 - val_acc: 0.4211\n",
      "Epoch 66/100\n",
      "529/529 [==============================] - 1s - loss: 2.7599 - acc: 0.4291 - val_loss: 3.6080 - val_acc: 0.4211\n",
      "Epoch 67/100\n",
      "529/529 [==============================] - 1s - loss: 2.7601 - acc: 0.4234 - val_loss: 3.5676 - val_acc: 0.4211\n",
      "Epoch 68/100\n",
      "529/529 [==============================] - 2s - loss: 2.7284 - acc: 0.4310 - val_loss: 3.5235 - val_acc: 0.4211\n",
      "Epoch 69/100\n",
      "529/529 [==============================] - 3s - loss: 2.7282 - acc: 0.4216 - val_loss: 3.5735 - val_acc: 0.4211\n",
      "Epoch 70/100\n",
      "529/529 [==============================] - 2s - loss: 2.7064 - acc: 0.4272 - val_loss: 3.5569 - val_acc: 0.4211\n",
      "Epoch 71/100\n",
      "529/529 [==============================] - 2s - loss: 2.7014 - acc: 0.4216 - val_loss: 3.5297 - val_acc: 0.4211\n",
      "Epoch 72/100\n",
      "529/529 [==============================] - 2s - loss: 2.6837 - acc: 0.4310 - val_loss: 3.5566 - val_acc: 0.4962\n",
      "Epoch 73/100\n",
      "529/529 [==============================] - 2s - loss: 2.6741 - acc: 0.4348 - val_loss: 3.6069 - val_acc: 0.4962\n",
      "Epoch 74/100\n",
      "529/529 [==============================] - 2s - loss: 2.6861 - acc: 0.4405 - val_loss: 3.5837 - val_acc: 0.4962\n",
      "Epoch 75/100\n",
      "529/529 [==============================] - 2s - loss: 2.6376 - acc: 0.4707 - val_loss: 3.5335 - val_acc: 0.4962\n",
      "Epoch 76/100\n",
      "529/529 [==============================] - 2s - loss: 2.6354 - acc: 0.4707 - val_loss: 3.5506 - val_acc: 0.4962\n",
      "Epoch 77/100\n",
      "529/529 [==============================] - 2s - loss: 2.6343 - acc: 0.4329 - val_loss: 3.6033 - val_acc: 0.4962\n",
      "Epoch 78/100\n",
      "529/529 [==============================] - 2s - loss: 2.6381 - acc: 0.4348 - val_loss: 3.5517 - val_acc: 0.4962\n",
      "Epoch 79/100\n",
      "529/529 [==============================] - 2s - loss: 2.6112 - acc: 0.4707 - val_loss: 3.5575 - val_acc: 0.4962\n",
      "Epoch 80/100\n",
      "529/529 [==============================] - 2s - loss: 2.5748 - acc: 0.4707 - val_loss: 3.5370 - val_acc: 0.4962\n",
      "Epoch 81/100\n",
      "529/529 [==============================] - 1s - loss: 2.5607 - acc: 0.4707 - val_loss: 3.5396 - val_acc: 0.4962\n",
      "Epoch 82/100\n",
      "529/529 [==============================] - 1s - loss: 2.5536 - acc: 0.4707 - val_loss: 3.5273 - val_acc: 0.4962\n",
      "Epoch 83/100\n",
      "529/529 [==============================] - 2s - loss: 2.6031 - acc: 0.4707 - val_loss: 3.5469 - val_acc: 0.4962\n",
      "Epoch 84/100\n",
      "529/529 [==============================] - 2s - loss: 2.5402 - acc: 0.4707 - val_loss: 3.5196 - val_acc: 0.4962\n",
      "Epoch 85/100\n",
      "529/529 [==============================] - 2s - loss: 2.5366 - acc: 0.4707 - val_loss: 3.5424 - val_acc: 0.4962\n",
      "Epoch 86/100\n",
      "529/529 [==============================] - 2s - loss: 2.5088 - acc: 0.4707 - val_loss: 3.5020 - val_acc: 0.4962\n",
      "Epoch 87/100\n",
      "529/529 [==============================] - 2s - loss: 2.4990 - acc: 0.4707 - val_loss: 3.5261 - val_acc: 0.4962\n",
      "Epoch 88/100\n",
      "529/529 [==============================] - 2s - loss: 2.4974 - acc: 0.4745 - val_loss: 3.5191 - val_acc: 0.4962\n",
      "Epoch 89/100\n",
      "529/529 [==============================] - 2s - loss: 2.4854 - acc: 0.4707 - val_loss: 3.5126 - val_acc: 0.4812\n",
      "Epoch 90/100\n",
      "529/529 [==============================] - 2s - loss: 2.4564 - acc: 0.4764 - val_loss: 3.5120 - val_acc: 0.4962\n",
      "Epoch 91/100\n",
      "529/529 [==============================] - 2s - loss: 2.4481 - acc: 0.4707 - val_loss: 3.5014 - val_acc: 0.4962\n",
      "Epoch 92/100\n",
      "529/529 [==============================] - 2s - loss: 2.4499 - acc: 0.4726 - val_loss: 3.5206 - val_acc: 0.4962\n",
      "Epoch 93/100\n",
      "529/529 [==============================] - 2s - loss: 2.4451 - acc: 0.4707 - val_loss: 3.5069 - val_acc: 0.4962\n",
      "Epoch 94/100\n",
      "529/529 [==============================] - 2s - loss: 2.4022 - acc: 0.4707 - val_loss: 3.5118 - val_acc: 0.4962\n",
      "Epoch 95/100\n",
      "529/529 [==============================] - 2s - loss: 2.3931 - acc: 0.4726 - val_loss: 3.4774 - val_acc: 0.4962\n",
      "Epoch 96/100\n",
      "529/529 [==============================] - 2s - loss: 2.4282 - acc: 0.4745 - val_loss: 3.5088 - val_acc: 0.4962\n",
      "Epoch 97/100\n",
      "529/529 [==============================] - 2s - loss: 2.3898 - acc: 0.4707 - val_loss: 3.4910 - val_acc: 0.4962\n",
      "Epoch 98/100\n",
      "529/529 [==============================] - 2s - loss: 2.3631 - acc: 0.4726 - val_loss: 3.4877 - val_acc: 0.4962\n",
      "Epoch 99/100\n",
      "529/529 [==============================] - 2s - loss: 2.3470 - acc: 0.4707 - val_loss: 3.5615 - val_acc: 0.4737\n",
      "Epoch 100/100\n",
      "529/529 [==============================] - 1s - loss: 2.3723 - acc: 0.4783 - val_loss: 3.5823 - val_acc: 0.4662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dfca7f1630>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "#Turn positive integers (indexes) into dense vectors of fixed size.\n",
    "trump_model = Sequential()\n",
    "trump_model.add(Embedding(input_dim=trump_input.shape[0],output_dim= 42, input_length=trump_input.shape[1]))\n",
    "# the model will take as input an integer matrix of size (batch, vocabulary_size).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, vocabulary_size, 42), where None is the batch dimension.\n",
    "\n",
    "#connect it to a dense output layer.\n",
    "trump_model.add(Flatten())\n",
    "trump_model.add(Dense(trump_output.shape[1], activation='sigmoid'))\n",
    "trump_model.summary()\n",
    "\n",
    "#training the model\n",
    "trump_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "trump_model.fit(trump_input, y=trump_output, batch_size=300, epochs=100, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Kim Kardashian Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 600, 42)           18942     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25200)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 600)               15120600  \n",
      "=================================================================\n",
      "Total params: 15,139,542\n",
      "Trainable params: 15,139,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 360 samples, validate on 91 samples\n",
      "Epoch 1/100\n",
      "360/360 [==============================] - 0s - loss: 6.2003 - acc: 0.0000e+00 - val_loss: 4.4808 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "360/360 [==============================] - 0s - loss: 4.1749 - acc: 0.0083 - val_loss: 4.1902 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "360/360 [==============================] - 1s - loss: 3.6775 - acc: 0.0167 - val_loss: 3.8751 - val_acc: 0.6484\n",
      "Epoch 4/100\n",
      "360/360 [==============================] - 0s - loss: 3.4227 - acc: 0.6056 - val_loss: 3.5844 - val_acc: 0.6484\n",
      "Epoch 5/100\n",
      "360/360 [==============================] - 1s - loss: 3.0646 - acc: 0.6056 - val_loss: 2.9336 - val_acc: 0.6484\n",
      "Epoch 6/100\n",
      "360/360 [==============================] - 1s - loss: 2.5470 - acc: 0.6056 - val_loss: 2.5126 - val_acc: 0.6484\n",
      "Epoch 7/100\n",
      "360/360 [==============================] - 1s - loss: 2.3351 - acc: 0.6056 - val_loss: 2.7258 - val_acc: 0.6484\n",
      "Epoch 8/100\n",
      "360/360 [==============================] - 1s - loss: 2.2972 - acc: 0.6056 - val_loss: 2.6002 - val_acc: 0.6484\n",
      "Epoch 9/100\n",
      "360/360 [==============================] - 1s - loss: 2.2850 - acc: 0.6056 - val_loss: 2.5995 - val_acc: 0.6484\n",
      "Epoch 10/100\n",
      "360/360 [==============================] - 1s - loss: 2.2685 - acc: 0.6056 - val_loss: 2.6668 - val_acc: 0.6484\n",
      "Epoch 11/100\n",
      "360/360 [==============================] - 1s - loss: 2.2588 - acc: 0.6056 - val_loss: 2.7064 - val_acc: 0.6484\n",
      "Epoch 12/100\n",
      "360/360 [==============================] - 1s - loss: 2.2567 - acc: 0.6056 - val_loss: 2.5275 - val_acc: 0.6484\n",
      "Epoch 13/100\n",
      "360/360 [==============================] - 1s - loss: 2.2394 - acc: 0.6056 - val_loss: 2.5717 - val_acc: 0.6484\n",
      "Epoch 14/100\n",
      "360/360 [==============================] - 1s - loss: 2.2568 - acc: 0.6056 - val_loss: 2.6138 - val_acc: 0.6484\n",
      "Epoch 15/100\n",
      "360/360 [==============================] - 1s - loss: 2.2015 - acc: 0.6056 - val_loss: 2.5855 - val_acc: 0.6484\n",
      "Epoch 16/100\n",
      "360/360 [==============================] - 1s - loss: 2.2314 - acc: 0.6056 - val_loss: 2.6591 - val_acc: 0.6484\n",
      "Epoch 17/100\n",
      "360/360 [==============================] - 1s - loss: 2.2060 - acc: 0.6056 - val_loss: 2.6847 - val_acc: 0.6484\n",
      "Epoch 18/100\n",
      "360/360 [==============================] - 1s - loss: 2.2071 - acc: 0.6056 - val_loss: 2.5496 - val_acc: 0.6484\n",
      "Epoch 19/100\n",
      "360/360 [==============================] - 1s - loss: 2.1956 - acc: 0.6056 - val_loss: 2.6077 - val_acc: 0.6484\n",
      "Epoch 20/100\n",
      "360/360 [==============================] - 1s - loss: 2.1989 - acc: 0.6056 - val_loss: 2.5788 - val_acc: 0.6484\n",
      "Epoch 21/100\n",
      "360/360 [==============================] - 1s - loss: 2.2191 - acc: 0.6056 - val_loss: 2.5538 - val_acc: 0.6484\n",
      "Epoch 22/100\n",
      "360/360 [==============================] - 1s - loss: 2.1952 - acc: 0.6056 - val_loss: 2.5284 - val_acc: 0.6484\n",
      "Epoch 23/100\n",
      "360/360 [==============================] - 1s - loss: 2.1965 - acc: 0.6056 - val_loss: 2.6777 - val_acc: 0.6484\n",
      "Epoch 24/100\n",
      "360/360 [==============================] - 1s - loss: 2.1854 - acc: 0.6056 - val_loss: 2.5834 - val_acc: 0.6484\n",
      "Epoch 25/100\n",
      "360/360 [==============================] - 1s - loss: 2.1961 - acc: 0.6056 - val_loss: 2.6717 - val_acc: 0.6484\n",
      "Epoch 26/100\n",
      "360/360 [==============================] - 1s - loss: 2.1968 - acc: 0.6056 - val_loss: 2.6305 - val_acc: 0.6484\n",
      "Epoch 27/100\n",
      "360/360 [==============================] - 1s - loss: 2.2114 - acc: 0.6056 - val_loss: 2.5911 - val_acc: 0.6484\n",
      "Epoch 28/100\n",
      "360/360 [==============================] - 1s - loss: 2.1603 - acc: 0.6056 - val_loss: 2.6821 - val_acc: 0.6484\n",
      "Epoch 29/100\n",
      "360/360 [==============================] - 1s - loss: 2.1567 - acc: 0.6056 - val_loss: 2.5863 - val_acc: 0.6484\n",
      "Epoch 30/100\n",
      "360/360 [==============================] - 1s - loss: 2.1703 - acc: 0.6056 - val_loss: 2.6752 - val_acc: 0.6484\n",
      "Epoch 31/100\n",
      "360/360 [==============================] - 1s - loss: 2.1693 - acc: 0.6056 - val_loss: 2.6184 - val_acc: 0.6484\n",
      "Epoch 32/100\n",
      "360/360 [==============================] - 0s - loss: 2.1478 - acc: 0.6056 - val_loss: 2.6748 - val_acc: 0.6484\n",
      "Epoch 33/100\n",
      "360/360 [==============================] - 0s - loss: 2.1412 - acc: 0.6056 - val_loss: 2.7241 - val_acc: 0.6484\n",
      "Epoch 34/100\n",
      "360/360 [==============================] - 0s - loss: 2.1465 - acc: 0.6056 - val_loss: 2.6144 - val_acc: 0.6484\n",
      "Epoch 35/100\n",
      "360/360 [==============================] - 0s - loss: 2.1372 - acc: 0.6056 - val_loss: 2.7062 - val_acc: 0.6484\n",
      "Epoch 36/100\n",
      "360/360 [==============================] - 0s - loss: 2.1559 - acc: 0.6056 - val_loss: 2.7411 - val_acc: 0.6484\n",
      "Epoch 37/100\n",
      "360/360 [==============================] - 0s - loss: 2.1526 - acc: 0.6056 - val_loss: 2.7013 - val_acc: 0.6484\n",
      "Epoch 38/100\n",
      "360/360 [==============================] - 0s - loss: 2.1271 - acc: 0.6056 - val_loss: 2.7078 - val_acc: 0.6484\n",
      "Epoch 39/100\n",
      "360/360 [==============================] - 0s - loss: 2.1433 - acc: 0.6056 - val_loss: 2.7246 - val_acc: 0.6484\n",
      "Epoch 40/100\n",
      "360/360 [==============================] - 0s - loss: 2.1376 - acc: 0.6056 - val_loss: 2.7227 - val_acc: 0.6484\n",
      "Epoch 41/100\n",
      "360/360 [==============================] - 0s - loss: 2.1161 - acc: 0.6056 - val_loss: 2.6289 - val_acc: 0.6484\n",
      "Epoch 42/100\n",
      "360/360 [==============================] - 0s - loss: 2.1211 - acc: 0.6056 - val_loss: 2.7178 - val_acc: 0.6484\n",
      "Epoch 43/100\n",
      "360/360 [==============================] - 0s - loss: 2.1210 - acc: 0.6056 - val_loss: 2.6707 - val_acc: 0.6484\n",
      "Epoch 44/100\n",
      "360/360 [==============================] - 0s - loss: 2.1080 - acc: 0.6056 - val_loss: 2.7448 - val_acc: 0.6484\n",
      "Epoch 45/100\n",
      "360/360 [==============================] - 1s - loss: 2.1201 - acc: 0.6056 - val_loss: 2.7582 - val_acc: 0.6484\n",
      "Epoch 46/100\n",
      "360/360 [==============================] - 0s - loss: 2.1217 - acc: 0.6056 - val_loss: 2.7634 - val_acc: 0.6484\n",
      "Epoch 47/100\n",
      "360/360 [==============================] - 0s - loss: 2.1023 - acc: 0.6056 - val_loss: 2.6596 - val_acc: 0.6484\n",
      "Epoch 48/100\n",
      "360/360 [==============================] - 0s - loss: 2.0991 - acc: 0.6056 - val_loss: 2.7209 - val_acc: 0.6484\n",
      "Epoch 49/100\n",
      "360/360 [==============================] - 0s - loss: 2.0961 - acc: 0.6056 - val_loss: 2.6657 - val_acc: 0.6484\n",
      "Epoch 50/100\n",
      "360/360 [==============================] - 1s - loss: 2.0658 - acc: 0.6056 - val_loss: 2.7244 - val_acc: 0.6484\n",
      "Epoch 51/100\n",
      "360/360 [==============================] - 1s - loss: 2.0747 - acc: 0.6056 - val_loss: 2.7586 - val_acc: 0.6484\n",
      "Epoch 52/100\n",
      "360/360 [==============================] - 1s - loss: 2.1100 - acc: 0.6056 - val_loss: 2.7821 - val_acc: 0.6484\n",
      "Epoch 53/100\n",
      "360/360 [==============================] - 1s - loss: 2.0924 - acc: 0.6056 - val_loss: 2.7462 - val_acc: 0.6484\n",
      "Epoch 54/100\n",
      "360/360 [==============================] - 1s - loss: 2.0938 - acc: 0.6056 - val_loss: 2.6457 - val_acc: 0.6484\n",
      "Epoch 55/100\n",
      "360/360 [==============================] - 0s - loss: 2.0785 - acc: 0.6056 - val_loss: 2.7152 - val_acc: 0.6484\n",
      "Epoch 56/100\n",
      "360/360 [==============================] - 0s - loss: 2.0671 - acc: 0.6056 - val_loss: 2.7213 - val_acc: 0.6484\n",
      "Epoch 57/100\n",
      "360/360 [==============================] - 0s - loss: 2.0531 - acc: 0.6056 - val_loss: 2.7055 - val_acc: 0.6484\n",
      "Epoch 58/100\n",
      "360/360 [==============================] - 0s - loss: 2.0412 - acc: 0.6056 - val_loss: 2.6884 - val_acc: 0.6484\n",
      "Epoch 59/100\n",
      "360/360 [==============================] - 0s - loss: 2.0412 - acc: 0.6056 - val_loss: 2.7202 - val_acc: 0.6484\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 0s - loss: 2.0630 - acc: 0.6056 - val_loss: 2.7428 - val_acc: 0.6484\n",
      "Epoch 61/100\n",
      "360/360 [==============================] - 0s - loss: 2.0436 - acc: 0.6056 - val_loss: 2.7116 - val_acc: 0.6484\n",
      "Epoch 62/100\n",
      "360/360 [==============================] - 0s - loss: 2.0341 - acc: 0.6056 - val_loss: 2.7291 - val_acc: 0.6484\n",
      "Epoch 63/100\n",
      "360/360 [==============================] - 0s - loss: 2.0334 - acc: 0.6056 - val_loss: 2.7283 - val_acc: 0.6484\n",
      "Epoch 64/100\n",
      "360/360 [==============================] - 0s - loss: 2.0358 - acc: 0.6056 - val_loss: 2.7170 - val_acc: 0.6484\n",
      "Epoch 65/100\n",
      "360/360 [==============================] - 0s - loss: 2.0300 - acc: 0.6056 - val_loss: 2.7142 - val_acc: 0.6484\n",
      "Epoch 66/100\n",
      "360/360 [==============================] - 0s - loss: 2.0422 - acc: 0.6056 - val_loss: 2.6759 - val_acc: 0.6484\n",
      "Epoch 67/100\n",
      "360/360 [==============================] - 0s - loss: 2.0216 - acc: 0.6056 - val_loss: 2.7557 - val_acc: 0.6484\n",
      "Epoch 68/100\n",
      "360/360 [==============================] - 0s - loss: 2.0149 - acc: 0.6056 - val_loss: 2.7283 - val_acc: 0.6484\n",
      "Epoch 69/100\n",
      "360/360 [==============================] - 0s - loss: 2.0178 - acc: 0.6056 - val_loss: 2.7410 - val_acc: 0.6484\n",
      "Epoch 70/100\n",
      "360/360 [==============================] - 0s - loss: 1.9978 - acc: 0.6056 - val_loss: 2.7906 - val_acc: 0.6484\n",
      "Epoch 71/100\n",
      "360/360 [==============================] - 0s - loss: 2.0079 - acc: 0.6056 - val_loss: 2.7697 - val_acc: 0.6484\n",
      "Epoch 72/100\n",
      "360/360 [==============================] - 0s - loss: 1.9973 - acc: 0.6056 - val_loss: 2.8097 - val_acc: 0.6484\n",
      "Epoch 73/100\n",
      "360/360 [==============================] - 0s - loss: 1.9896 - acc: 0.6056 - val_loss: 2.7997 - val_acc: 0.6484\n",
      "Epoch 74/100\n",
      "360/360 [==============================] - 0s - loss: 1.9795 - acc: 0.6056 - val_loss: 2.7146 - val_acc: 0.6484\n",
      "Epoch 75/100\n",
      "360/360 [==============================] - 0s - loss: 1.9877 - acc: 0.6056 - val_loss: 2.7744 - val_acc: 0.6484\n",
      "Epoch 76/100\n",
      "360/360 [==============================] - 0s - loss: 1.9721 - acc: 0.6056 - val_loss: 2.6790 - val_acc: 0.6484\n",
      "Epoch 77/100\n",
      "360/360 [==============================] - 0s - loss: 1.9748 - acc: 0.6056 - val_loss: 2.6896 - val_acc: 0.6484\n",
      "Epoch 78/100\n",
      "360/360 [==============================] - 0s - loss: 1.9598 - acc: 0.6056 - val_loss: 2.7624 - val_acc: 0.6484\n",
      "Epoch 79/100\n",
      "360/360 [==============================] - 0s - loss: 1.9655 - acc: 0.6056 - val_loss: 2.7672 - val_acc: 0.6484\n",
      "Epoch 80/100\n",
      "360/360 [==============================] - 0s - loss: 1.9548 - acc: 0.6056 - val_loss: 2.7434 - val_acc: 0.6484\n",
      "Epoch 81/100\n",
      "360/360 [==============================] - 0s - loss: 1.9607 - acc: 0.6056 - val_loss: 2.7583 - val_acc: 0.6484\n",
      "Epoch 82/100\n",
      "360/360 [==============================] - 0s - loss: 1.9411 - acc: 0.6056 - val_loss: 2.7858 - val_acc: 0.6484\n",
      "Epoch 83/100\n",
      "360/360 [==============================] - 0s - loss: 1.9473 - acc: 0.6056 - val_loss: 2.7631 - val_acc: 0.6484\n",
      "Epoch 84/100\n",
      "360/360 [==============================] - 0s - loss: 1.9475 - acc: 0.6056 - val_loss: 2.7455 - val_acc: 0.6484\n",
      "Epoch 85/100\n",
      "360/360 [==============================] - 0s - loss: 1.9380 - acc: 0.6056 - val_loss: 2.7394 - val_acc: 0.6484\n",
      "Epoch 86/100\n",
      "360/360 [==============================] - 0s - loss: 1.9360 - acc: 0.6056 - val_loss: 2.7362 - val_acc: 0.6484\n",
      "Epoch 87/100\n",
      "360/360 [==============================] - 0s - loss: 1.9124 - acc: 0.6056 - val_loss: 2.7599 - val_acc: 0.6484\n",
      "Epoch 88/100\n",
      "360/360 [==============================] - 0s - loss: 1.9495 - acc: 0.6056 - val_loss: 2.7143 - val_acc: 0.6484\n",
      "Epoch 89/100\n",
      "360/360 [==============================] - 1s - loss: 1.9271 - acc: 0.6056 - val_loss: 2.7236 - val_acc: 0.6484\n",
      "Epoch 90/100\n",
      "360/360 [==============================] - 1s - loss: 1.9072 - acc: 0.6056 - val_loss: 2.7528 - val_acc: 0.6484\n",
      "Epoch 91/100\n",
      "360/360 [==============================] - 1s - loss: 1.8918 - acc: 0.6056 - val_loss: 2.7682 - val_acc: 0.6484\n",
      "Epoch 92/100\n",
      "360/360 [==============================] - 1s - loss: 1.9060 - acc: 0.6056 - val_loss: 2.7428 - val_acc: 0.6484\n",
      "Epoch 93/100\n",
      "360/360 [==============================] - 1s - loss: 1.8942 - acc: 0.6056 - val_loss: 2.7325 - val_acc: 0.6484\n",
      "Epoch 94/100\n",
      "360/360 [==============================] - 0s - loss: 1.8771 - acc: 0.6056 - val_loss: 2.6982 - val_acc: 0.6484\n",
      "Epoch 95/100\n",
      "360/360 [==============================] - 0s - loss: 1.8811 - acc: 0.6056 - val_loss: 2.7486 - val_acc: 0.6484\n",
      "Epoch 96/100\n",
      "360/360 [==============================] - 0s - loss: 1.8792 - acc: 0.6056 - val_loss: 2.7391 - val_acc: 0.6484\n",
      "Epoch 97/100\n",
      "360/360 [==============================] - 0s - loss: 1.8663 - acc: 0.6056 - val_loss: 2.7048 - val_acc: 0.6484\n",
      "Epoch 98/100\n",
      "360/360 [==============================] - 0s - loss: 1.8690 - acc: 0.6056 - val_loss: 2.7197 - val_acc: 0.6484\n",
      "Epoch 99/100\n",
      "360/360 [==============================] - 0s - loss: 1.8565 - acc: 0.6056 - val_loss: 2.7581 - val_acc: 0.6484\n",
      "Epoch 100/100\n",
      "360/360 [==============================] - 0s - loss: 1.8619 - acc: 0.6056 - val_loss: 2.7671 - val_acc: 0.6484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dfcc54c6a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turn positive integers (indexes) into dense vectors of fixed size.\n",
    "kim_model = Sequential()\n",
    "kim_model.add(Embedding(input_dim=kim_input.shape[0],output_dim= 42, input_length=kim_input.shape[1]))\n",
    "# the model will take as input an integer matrix of size (batch, vocabulary_size).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, vocabulary_size, 42), where None is the batch dimension.\n",
    "\n",
    "#connect it to a dense output layer.\n",
    "kim_model.add(Flatten())\n",
    "kim_model.add(Dense(kim_output.shape[1], activation='sigmoid'))\n",
    "kim_model.summary()\n",
    "\n",
    "#training the model\n",
    "kim_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "kim_model.fit(kim_input, y=kim_output, batch_size=300, epochs=100, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Katy Perry Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 600, 42)           25830     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25200)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 600)               15120600  \n",
      "=================================================================\n",
      "Total params: 15,146,430\n",
      "Trainable params: 15,146,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 492 samples, validate on 123 samples\n",
      "Epoch 1/100\n",
      "492/492 [==============================] - 1s - loss: 5.7721 - acc: 0.0041 - val_loss: 4.1672 - val_acc: 0.0244\n",
      "Epoch 2/100\n",
      "492/492 [==============================] - 1s - loss: 3.8444 - acc: 0.0122 - val_loss: 3.2768 - val_acc: 0.0244\n",
      "Epoch 3/100\n",
      "492/492 [==============================] - 1s - loss: 3.1750 - acc: 0.2846 - val_loss: 2.6658 - val_acc: 0.7317\n",
      "Epoch 4/100\n",
      "492/492 [==============================] - 1s - loss: 2.6906 - acc: 0.6545 - val_loss: 2.2918 - val_acc: 0.7317\n",
      "Epoch 5/100\n",
      "492/492 [==============================] - 1s - loss: 2.2794 - acc: 0.6545 - val_loss: 1.9018 - val_acc: 0.7317\n",
      "Epoch 6/100\n",
      "492/492 [==============================] - 1s - loss: 2.0373 - acc: 0.6545 - val_loss: 1.8833 - val_acc: 0.7317\n",
      "Epoch 7/100\n",
      "492/492 [==============================] - 1s - loss: 2.0638 - acc: 0.6545 - val_loss: 1.8468 - val_acc: 0.7317\n",
      "Epoch 8/100\n",
      "492/492 [==============================] - 1s - loss: 2.0098 - acc: 0.6545 - val_loss: 1.8892 - val_acc: 0.7317\n",
      "Epoch 9/100\n",
      "492/492 [==============================] - 1s - loss: 2.0159 - acc: 0.6545 - val_loss: 1.8412 - val_acc: 0.7317\n",
      "Epoch 10/100\n",
      "492/492 [==============================] - 1s - loss: 2.0139 - acc: 0.6545 - val_loss: 1.8158 - val_acc: 0.7317\n",
      "Epoch 11/100\n",
      "492/492 [==============================] - 1s - loss: 2.0077 - acc: 0.6545 - val_loss: 1.9002 - val_acc: 0.7317\n",
      "Epoch 12/100\n",
      "492/492 [==============================] - 1s - loss: 2.0162 - acc: 0.6545 - val_loss: 1.8390 - val_acc: 0.7317\n",
      "Epoch 13/100\n",
      "492/492 [==============================] - 1s - loss: 1.9878 - acc: 0.6545 - val_loss: 1.8674 - val_acc: 0.7317\n",
      "Epoch 14/100\n",
      "492/492 [==============================] - 1s - loss: 1.9642 - acc: 0.6545 - val_loss: 1.8707 - val_acc: 0.7317\n",
      "Epoch 15/100\n",
      "492/492 [==============================] - 1s - loss: 1.9923 - acc: 0.6545 - val_loss: 1.8675 - val_acc: 0.7317\n",
      "Epoch 16/100\n",
      "492/492 [==============================] - 1s - loss: 1.9943 - acc: 0.6545 - val_loss: 1.8736 - val_acc: 0.7317\n",
      "Epoch 17/100\n",
      "492/492 [==============================] - 1s - loss: 1.9910 - acc: 0.6545 - val_loss: 1.9288 - val_acc: 0.7317\n",
      "Epoch 18/100\n",
      "492/492 [==============================] - 1s - loss: 1.9816 - acc: 0.6545 - val_loss: 1.8747 - val_acc: 0.7317\n",
      "Epoch 19/100\n",
      "492/492 [==============================] - 1s - loss: 1.9480 - acc: 0.6545 - val_loss: 1.8863 - val_acc: 0.7317\n",
      "Epoch 20/100\n",
      "492/492 [==============================] - 1s - loss: 1.9641 - acc: 0.6545 - val_loss: 1.9046 - val_acc: 0.7317\n",
      "Epoch 21/100\n",
      "492/492 [==============================] - 1s - loss: 1.9685 - acc: 0.6545 - val_loss: 1.9008 - val_acc: 0.7317\n",
      "Epoch 22/100\n",
      "492/492 [==============================] - 1s - loss: 1.9660 - acc: 0.6545 - val_loss: 1.8555 - val_acc: 0.7317\n",
      "Epoch 23/100\n",
      "492/492 [==============================] - 1s - loss: 1.9585 - acc: 0.6545 - val_loss: 1.8333 - val_acc: 0.7317\n",
      "Epoch 24/100\n",
      "492/492 [==============================] - 1s - loss: 1.9525 - acc: 0.6545 - val_loss: 1.8522 - val_acc: 0.7317\n",
      "Epoch 25/100\n",
      "492/492 [==============================] - 1s - loss: 1.9644 - acc: 0.6545 - val_loss: 1.9156 - val_acc: 0.7317\n",
      "Epoch 26/100\n",
      "492/492 [==============================] - 1s - loss: 1.9482 - acc: 0.6545 - val_loss: 1.8981 - val_acc: 0.7317\n",
      "Epoch 27/100\n",
      "492/492 [==============================] - 1s - loss: 1.9379 - acc: 0.6545 - val_loss: 1.8931 - val_acc: 0.7317\n",
      "Epoch 28/100\n",
      "492/492 [==============================] - 1s - loss: 1.9627 - acc: 0.6545 - val_loss: 1.8648 - val_acc: 0.7317\n",
      "Epoch 29/100\n",
      "492/492 [==============================] - 1s - loss: 1.9319 - acc: 0.6545 - val_loss: 1.8846 - val_acc: 0.7317\n",
      "Epoch 30/100\n",
      "492/492 [==============================] - 1s - loss: 1.9233 - acc: 0.6545 - val_loss: 1.8772 - val_acc: 0.7317\n",
      "Epoch 31/100\n",
      "492/492 [==============================] - 1s - loss: 1.9378 - acc: 0.6545 - val_loss: 1.9228 - val_acc: 0.7317\n",
      "Epoch 32/100\n",
      "492/492 [==============================] - 1s - loss: 1.9347 - acc: 0.6545 - val_loss: 1.9435 - val_acc: 0.7317\n",
      "Epoch 33/100\n",
      "492/492 [==============================] - 1s - loss: 1.9199 - acc: 0.6545 - val_loss: 1.9333 - val_acc: 0.7317\n",
      "Epoch 34/100\n",
      "492/492 [==============================] - 1s - loss: 1.9458 - acc: 0.6545 - val_loss: 1.8952 - val_acc: 0.7317\n",
      "Epoch 35/100\n",
      "492/492 [==============================] - 1s - loss: 1.9230 - acc: 0.6545 - val_loss: 1.9095 - val_acc: 0.7317\n",
      "Epoch 36/100\n",
      "492/492 [==============================] - 1s - loss: 1.9117 - acc: 0.6545 - val_loss: 1.8826 - val_acc: 0.7317\n",
      "Epoch 37/100\n",
      "492/492 [==============================] - 1s - loss: 1.9284 - acc: 0.6545 - val_loss: 1.9691 - val_acc: 0.7317\n",
      "Epoch 38/100\n",
      "492/492 [==============================] - 1s - loss: 1.9035 - acc: 0.6545 - val_loss: 1.9227 - val_acc: 0.7317\n",
      "Epoch 39/100\n",
      "492/492 [==============================] - 1s - loss: 1.9073 - acc: 0.6545 - val_loss: 1.9717 - val_acc: 0.7317\n",
      "Epoch 40/100\n",
      "492/492 [==============================] - 1s - loss: 1.9075 - acc: 0.6545 - val_loss: 1.9167 - val_acc: 0.7317\n",
      "Epoch 41/100\n",
      "492/492 [==============================] - 1s - loss: 1.9035 - acc: 0.6545 - val_loss: 1.9069 - val_acc: 0.7317\n",
      "Epoch 42/100\n",
      "492/492 [==============================] - 1s - loss: 1.9007 - acc: 0.6545 - val_loss: 1.9714 - val_acc: 0.7317\n",
      "Epoch 43/100\n",
      "492/492 [==============================] - 1s - loss: 1.8910 - acc: 0.6545 - val_loss: 1.9009 - val_acc: 0.7317\n",
      "Epoch 44/100\n",
      "492/492 [==============================] - 1s - loss: 1.8816 - acc: 0.6545 - val_loss: 1.9216 - val_acc: 0.7317\n",
      "Epoch 45/100\n",
      "492/492 [==============================] - 1s - loss: 1.8738 - acc: 0.6545 - val_loss: 1.9297 - val_acc: 0.7317\n",
      "Epoch 46/100\n",
      "492/492 [==============================] - 1s - loss: 1.8951 - acc: 0.6545 - val_loss: 1.9549 - val_acc: 0.7317\n",
      "Epoch 47/100\n",
      "492/492 [==============================] - 1s - loss: 1.8886 - acc: 0.6545 - val_loss: 1.9421 - val_acc: 0.7317\n",
      "Epoch 48/100\n",
      "492/492 [==============================] - 1s - loss: 1.8776 - acc: 0.6545 - val_loss: 1.9341 - val_acc: 0.7317\n",
      "Epoch 49/100\n",
      "492/492 [==============================] - 1s - loss: 1.8743 - acc: 0.6545 - val_loss: 1.9371 - val_acc: 0.7317\n",
      "Epoch 50/100\n",
      "492/492 [==============================] - 1s - loss: 1.8707 - acc: 0.6545 - val_loss: 1.8879 - val_acc: 0.7317\n",
      "Epoch 51/100\n",
      "492/492 [==============================] - 1s - loss: 1.8862 - acc: 0.6545 - val_loss: 1.9406 - val_acc: 0.7317\n",
      "Epoch 52/100\n",
      "492/492 [==============================] - 1s - loss: 1.8663 - acc: 0.6545 - val_loss: 1.9421 - val_acc: 0.7317\n",
      "Epoch 53/100\n",
      "492/492 [==============================] - 1s - loss: 1.8533 - acc: 0.6545 - val_loss: 1.9259 - val_acc: 0.7317\n",
      "Epoch 54/100\n",
      "492/492 [==============================] - 1s - loss: 1.8620 - acc: 0.6545 - val_loss: 1.9830 - val_acc: 0.7317\n",
      "Epoch 55/100\n",
      "492/492 [==============================] - 1s - loss: 1.8475 - acc: 0.6545 - val_loss: 1.9642 - val_acc: 0.7317\n",
      "Epoch 56/100\n",
      "492/492 [==============================] - 1s - loss: 1.8486 - acc: 0.6545 - val_loss: 1.9618 - val_acc: 0.7317\n",
      "Epoch 57/100\n",
      "492/492 [==============================] - 1s - loss: 1.8482 - acc: 0.6545 - val_loss: 1.9369 - val_acc: 0.7317\n",
      "Epoch 58/100\n",
      "492/492 [==============================] - 1s - loss: 1.8443 - acc: 0.6545 - val_loss: 1.9744 - val_acc: 0.7317\n",
      "Epoch 59/100\n",
      "492/492 [==============================] - 1s - loss: 1.8445 - acc: 0.6545 - val_loss: 2.0018 - val_acc: 0.7317\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492/492 [==============================] - 1s - loss: 1.8285 - acc: 0.6545 - val_loss: 1.9558 - val_acc: 0.7317\n",
      "Epoch 61/100\n",
      "492/492 [==============================] - 1s - loss: 1.8385 - acc: 0.6545 - val_loss: 1.9784 - val_acc: 0.7317\n",
      "Epoch 62/100\n",
      "492/492 [==============================] - 1s - loss: 1.8209 - acc: 0.6545 - val_loss: 1.9490 - val_acc: 0.7317\n",
      "Epoch 63/100\n",
      "492/492 [==============================] - 1s - loss: 1.8267 - acc: 0.6545 - val_loss: 2.0043 - val_acc: 0.7317\n",
      "Epoch 64/100\n",
      "492/492 [==============================] - 1s - loss: 1.8141 - acc: 0.6545 - val_loss: 1.9858 - val_acc: 0.7317\n",
      "Epoch 65/100\n",
      "492/492 [==============================] - 1s - loss: 1.8011 - acc: 0.6545 - val_loss: 1.9771 - val_acc: 0.7317\n",
      "Epoch 66/100\n",
      "492/492 [==============================] - 1s - loss: 1.8018 - acc: 0.6545 - val_loss: 1.9948 - val_acc: 0.7317\n",
      "Epoch 67/100\n",
      "492/492 [==============================] - 1s - loss: 1.7999 - acc: 0.6545 - val_loss: 2.0030 - val_acc: 0.7317\n",
      "Epoch 68/100\n",
      "492/492 [==============================] - 1s - loss: 1.8061 - acc: 0.6545 - val_loss: 2.0153 - val_acc: 0.7317\n",
      "Epoch 69/100\n",
      "492/492 [==============================] - 1s - loss: 1.7931 - acc: 0.6545 - val_loss: 1.9800 - val_acc: 0.7317\n",
      "Epoch 70/100\n",
      "492/492 [==============================] - 1s - loss: 1.7915 - acc: 0.6545 - val_loss: 1.9787 - val_acc: 0.7317\n",
      "Epoch 71/100\n",
      "492/492 [==============================] - 1s - loss: 1.7884 - acc: 0.6545 - val_loss: 1.9865 - val_acc: 0.7317\n",
      "Epoch 72/100\n",
      "492/492 [==============================] - 1s - loss: 1.7840 - acc: 0.6545 - val_loss: 2.0003 - val_acc: 0.7317\n",
      "Epoch 73/100\n",
      "492/492 [==============================] - 1s - loss: 1.7817 - acc: 0.6545 - val_loss: 1.9844 - val_acc: 0.7317\n",
      "Epoch 74/100\n",
      "492/492 [==============================] - 1s - loss: 1.7785 - acc: 0.6545 - val_loss: 1.9815 - val_acc: 0.7317\n",
      "Epoch 75/100\n",
      "492/492 [==============================] - 1s - loss: 1.7685 - acc: 0.6545 - val_loss: 1.9740 - val_acc: 0.7317\n",
      "Epoch 76/100\n",
      "492/492 [==============================] - 1s - loss: 1.7708 - acc: 0.6545 - val_loss: 1.9684 - val_acc: 0.7317\n",
      "Epoch 77/100\n",
      "492/492 [==============================] - 1s - loss: 1.7693 - acc: 0.6545 - val_loss: 1.9538 - val_acc: 0.7317\n",
      "Epoch 78/100\n",
      "492/492 [==============================] - 1s - loss: 1.7894 - acc: 0.6545 - val_loss: 1.9761 - val_acc: 0.7317\n",
      "Epoch 79/100\n",
      "492/492 [==============================] - 1s - loss: 1.7405 - acc: 0.6545 - val_loss: 2.0002 - val_acc: 0.7317\n",
      "Epoch 80/100\n",
      "492/492 [==============================] - 1s - loss: 1.7386 - acc: 0.6545 - val_loss: 1.9514 - val_acc: 0.7317\n",
      "Epoch 81/100\n",
      "492/492 [==============================] - 1s - loss: 1.7339 - acc: 0.6545 - val_loss: 1.9597 - val_acc: 0.7317\n",
      "Epoch 82/100\n",
      "492/492 [==============================] - 1s - loss: 1.7482 - acc: 0.6545 - val_loss: 1.9678 - val_acc: 0.7317\n",
      "Epoch 83/100\n",
      "492/492 [==============================] - 1s - loss: 1.7526 - acc: 0.6545 - val_loss: 1.9917 - val_acc: 0.7317\n",
      "Epoch 84/100\n",
      "492/492 [==============================] - 1s - loss: 1.7178 - acc: 0.6545 - val_loss: 1.9527 - val_acc: 0.7317\n",
      "Epoch 85/100\n",
      "492/492 [==============================] - 1s - loss: 1.7132 - acc: 0.6545 - val_loss: 1.9663 - val_acc: 0.7317\n",
      "Epoch 86/100\n",
      "492/492 [==============================] - 1s - loss: 1.7111 - acc: 0.6545 - val_loss: 1.9777 - val_acc: 0.7317\n",
      "Epoch 87/100\n",
      "492/492 [==============================] - 1s - loss: 1.7004 - acc: 0.6545 - val_loss: 1.9701 - val_acc: 0.7317\n",
      "Epoch 88/100\n",
      "492/492 [==============================] - 1s - loss: 1.7025 - acc: 0.6545 - val_loss: 1.9260 - val_acc: 0.7317\n",
      "Epoch 89/100\n",
      "492/492 [==============================] - 1s - loss: 1.7055 - acc: 0.6545 - val_loss: 1.9783 - val_acc: 0.7317\n",
      "Epoch 90/100\n",
      "492/492 [==============================] - 1s - loss: 1.7001 - acc: 0.6545 - val_loss: 2.0566 - val_acc: 0.7317\n",
      "Epoch 91/100\n",
      "492/492 [==============================] - 1s - loss: 1.7187 - acc: 0.6545 - val_loss: 1.9402 - val_acc: 0.7317\n",
      "Epoch 92/100\n",
      "492/492 [==============================] - 1s - loss: 1.6653 - acc: 0.6545 - val_loss: 1.9833 - val_acc: 0.7317\n",
      "Epoch 93/100\n",
      "492/492 [==============================] - 1s - loss: 1.6757 - acc: 0.6545 - val_loss: 2.0201 - val_acc: 0.7317\n",
      "Epoch 94/100\n",
      "492/492 [==============================] - 1s - loss: 1.6936 - acc: 0.6545 - val_loss: 1.9791 - val_acc: 0.7317\n",
      "Epoch 95/100\n",
      "492/492 [==============================] - 1s - loss: 1.6774 - acc: 0.6545 - val_loss: 1.9797 - val_acc: 0.7317\n",
      "Epoch 96/100\n",
      "492/492 [==============================] - 1s - loss: 1.6694 - acc: 0.6545 - val_loss: 1.9957 - val_acc: 0.7317\n",
      "Epoch 97/100\n",
      "492/492 [==============================] - 1s - loss: 1.6403 - acc: 0.6545 - val_loss: 1.9246 - val_acc: 0.7317\n",
      "Epoch 98/100\n",
      "492/492 [==============================] - 1s - loss: 1.6466 - acc: 0.6545 - val_loss: 1.9143 - val_acc: 0.7317\n",
      "Epoch 99/100\n",
      "492/492 [==============================] - 1s - loss: 1.6433 - acc: 0.6545 - val_loss: 1.9582 - val_acc: 0.7317\n",
      "Epoch 100/100\n",
      "492/492 [==============================] - 1s - loss: 1.6352 - acc: 0.6545 - val_loss: 1.9794 - val_acc: 0.7317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dfcc945a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turn positive integers (indexes) into dense vectors of fixed size.\n",
    "katy_model = Sequential()\n",
    "katy_model.add(Embedding(input_dim=katy_input.shape[0],output_dim= 42, input_length=katy_input.shape[1]))\n",
    "# the model will take as input an integer matrix of size (batch, vocabulary_size).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, vocabulary_size, 42), where None is the batch dimension.\n",
    "\n",
    "#connect it to a dense output layer.\n",
    "katy_model.add(Flatten())\n",
    "katy_model.add(Dense(katy_output.shape[1], activation='sigmoid'))\n",
    "katy_model.summary()\n",
    "\n",
    "#training the model\n",
    "katy_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "katy_model.fit(katy_input, y=katy_output, batch_size=300, epochs=100, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bill Gates Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 600, 42)           4620      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25200)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 600)               15120600  \n",
      "=================================================================\n",
      "Total params: 15,125,220\n",
      "Trainable params: 15,125,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 88 samples, validate on 22 samples\n",
      "Epoch 1/100\n",
      "88/88 [==============================] - 0s - loss: 6.4021 - acc: 0.0000e+00 - val_loss: 4.4669 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "88/88 [==============================] - 0s - loss: 4.3712 - acc: 0.0568 - val_loss: 2.8734 - val_acc: 0.9091\n",
      "Epoch 3/100\n",
      "88/88 [==============================] - 0s - loss: 2.6769 - acc: 0.8182 - val_loss: 2.3434 - val_acc: 0.9091\n",
      "Epoch 4/100\n",
      "88/88 [==============================] - 0s - loss: 2.0723 - acc: 0.8182 - val_loss: 2.0077 - val_acc: 0.9091\n",
      "Epoch 5/100\n",
      "88/88 [==============================] - 0s - loss: 1.7414 - acc: 0.8182 - val_loss: 1.4544 - val_acc: 0.9091\n",
      "Epoch 6/100\n",
      "88/88 [==============================] - 0s - loss: 1.2989 - acc: 0.8182 - val_loss: 1.0312 - val_acc: 0.9091\n",
      "Epoch 7/100\n",
      "88/88 [==============================] - 0s - loss: 0.9878 - acc: 0.8182 - val_loss: 0.8197 - val_acc: 0.9091\n",
      "Epoch 8/100\n",
      "88/88 [==============================] - 0s - loss: 0.8794 - acc: 0.8182 - val_loss: 0.8315 - val_acc: 0.9091\n",
      "Epoch 9/100\n",
      "88/88 [==============================] - 0s - loss: 0.8373 - acc: 0.8182 - val_loss: 0.8386 - val_acc: 0.9091\n",
      "Epoch 10/100\n",
      "88/88 [==============================] - 0s - loss: 0.8218 - acc: 0.8182 - val_loss: 0.8374 - val_acc: 0.9091\n",
      "Epoch 11/100\n",
      "88/88 [==============================] - 0s - loss: 0.8144 - acc: 0.8182 - val_loss: 0.8461 - val_acc: 0.9091\n",
      "Epoch 12/100\n",
      "88/88 [==============================] - 0s - loss: 0.8095 - acc: 0.8182 - val_loss: 0.8487 - val_acc: 0.9091\n",
      "Epoch 13/100\n",
      "88/88 [==============================] - 0s - loss: 0.8079 - acc: 0.8182 - val_loss: 0.8575 - val_acc: 0.9091\n",
      "Epoch 14/100\n",
      "88/88 [==============================] - 0s - loss: 0.8089 - acc: 0.8182 - val_loss: 0.8689 - val_acc: 0.9091\n",
      "Epoch 15/100\n",
      "88/88 [==============================] - 0s - loss: 0.8143 - acc: 0.8182 - val_loss: 0.8757 - val_acc: 0.9091\n",
      "Epoch 16/100\n",
      "88/88 [==============================] - 0s - loss: 0.8268 - acc: 0.8182 - val_loss: 0.8706 - val_acc: 0.9091\n",
      "Epoch 17/100\n",
      "88/88 [==============================] - 0s - loss: 0.8198 - acc: 0.8182 - val_loss: 0.8996 - val_acc: 0.9091\n",
      "Epoch 18/100\n",
      "88/88 [==============================] - 0s - loss: 0.8298 - acc: 0.8182 - val_loss: 0.8471 - val_acc: 0.9091\n",
      "Epoch 19/100\n",
      "88/88 [==============================] - 0s - loss: 0.8243 - acc: 0.8182 - val_loss: 0.9136 - val_acc: 0.9091\n",
      "Epoch 20/100\n",
      "88/88 [==============================] - 0s - loss: 0.8171 - acc: 0.8182 - val_loss: 0.8582 - val_acc: 0.9091\n",
      "Epoch 21/100\n",
      "88/88 [==============================] - 0s - loss: 0.8155 - acc: 0.8182 - val_loss: 0.9073 - val_acc: 0.9091\n",
      "Epoch 22/100\n",
      "88/88 [==============================] - 0s - loss: 0.8096 - acc: 0.8182 - val_loss: 0.8707 - val_acc: 0.9091\n",
      "Epoch 23/100\n",
      "88/88 [==============================] - 0s - loss: 0.8068 - acc: 0.8182 - val_loss: 0.9129 - val_acc: 0.9091\n",
      "Epoch 24/100\n",
      "88/88 [==============================] - 0s - loss: 0.8070 - acc: 0.8182 - val_loss: 0.8777 - val_acc: 0.9091\n",
      "Epoch 25/100\n",
      "88/88 [==============================] - 0s - loss: 0.8061 - acc: 0.8182 - val_loss: 0.9205 - val_acc: 0.9091\n",
      "Epoch 26/100\n",
      "88/88 [==============================] - 0s - loss: 0.8070 - acc: 0.8182 - val_loss: 0.8835 - val_acc: 0.9091\n",
      "Epoch 27/100\n",
      "88/88 [==============================] - 0s - loss: 0.8064 - acc: 0.8182 - val_loss: 0.9277 - val_acc: 0.9091\n",
      "Epoch 28/100\n",
      "88/88 [==============================] - 0s - loss: 0.8074 - acc: 0.8182 - val_loss: 0.8887 - val_acc: 0.9091\n",
      "Epoch 29/100\n",
      "88/88 [==============================] - 0s - loss: 0.8062 - acc: 0.8182 - val_loss: 0.9337 - val_acc: 0.9091\n",
      "Epoch 30/100\n",
      "88/88 [==============================] - 0s - loss: 0.8069 - acc: 0.8182 - val_loss: 0.8937 - val_acc: 0.9091\n",
      "Epoch 31/100\n",
      "88/88 [==============================] - 0s - loss: 0.8052 - acc: 0.8182 - val_loss: 0.9382 - val_acc: 0.9091\n",
      "Epoch 32/100\n",
      "88/88 [==============================] - 0s - loss: 0.8056 - acc: 0.8182 - val_loss: 0.8988 - val_acc: 0.9091\n",
      "Epoch 33/100\n",
      "88/88 [==============================] - 0s - loss: 0.8038 - acc: 0.8182 - val_loss: 0.9423 - val_acc: 0.9091\n",
      "Epoch 34/100\n",
      "88/88 [==============================] - 0s - loss: 0.8042 - acc: 0.8182 - val_loss: 0.9039 - val_acc: 0.9091\n",
      "Epoch 35/100\n",
      "88/88 [==============================] - 0s - loss: 0.8026 - acc: 0.8182 - val_loss: 0.9467 - val_acc: 0.9091\n",
      "Epoch 36/100\n",
      "88/88 [==============================] - 0s - loss: 0.8032 - acc: 0.8182 - val_loss: 0.9086 - val_acc: 0.9091\n",
      "Epoch 37/100\n",
      "88/88 [==============================] - 0s - loss: 0.8018 - acc: 0.8182 - val_loss: 0.9511 - val_acc: 0.9091\n",
      "Epoch 38/100\n",
      "88/88 [==============================] - 0s - loss: 0.8025 - acc: 0.8182 - val_loss: 0.9129 - val_acc: 0.9091\n",
      "Epoch 39/100\n",
      "88/88 [==============================] - 0s - loss: 0.8011 - acc: 0.8182 - val_loss: 0.9553 - val_acc: 0.9091\n",
      "Epoch 40/100\n",
      "88/88 [==============================] - 0s - loss: 0.8018 - acc: 0.8182 - val_loss: 0.9170 - val_acc: 0.9091\n",
      "Epoch 41/100\n",
      "88/88 [==============================] - 0s - loss: 0.8004 - acc: 0.8182 - val_loss: 0.9591 - val_acc: 0.9091\n",
      "Epoch 42/100\n",
      "88/88 [==============================] - 0s - loss: 0.8010 - acc: 0.8182 - val_loss: 0.9208 - val_acc: 0.9091\n",
      "Epoch 43/100\n",
      "88/88 [==============================] - 0s - loss: 0.7996 - acc: 0.8182 - val_loss: 0.9625 - val_acc: 0.9091\n",
      "Epoch 44/100\n",
      "88/88 [==============================] - 0s - loss: 0.8001 - acc: 0.8182 - val_loss: 0.9245 - val_acc: 0.9091\n",
      "Epoch 45/100\n",
      "88/88 [==============================] - 0s - loss: 0.7986 - acc: 0.8182 - val_loss: 0.9657 - val_acc: 0.9091\n",
      "Epoch 46/100\n",
      "88/88 [==============================] - 0s - loss: 0.7992 - acc: 0.8182 - val_loss: 0.9280 - val_acc: 0.9091\n",
      "Epoch 47/100\n",
      "88/88 [==============================] - 0s - loss: 0.7977 - acc: 0.8182 - val_loss: 0.9687 - val_acc: 0.9091\n",
      "Epoch 48/100\n",
      "88/88 [==============================] - 0s - loss: 0.7983 - acc: 0.8182 - val_loss: 0.9313 - val_acc: 0.9091\n",
      "Epoch 49/100\n",
      "88/88 [==============================] - 0s - loss: 0.7968 - acc: 0.8182 - val_loss: 0.9716 - val_acc: 0.9091\n",
      "Epoch 50/100\n",
      "88/88 [==============================] - 0s - loss: 0.7975 - acc: 0.8182 - val_loss: 0.9345 - val_acc: 0.9091\n",
      "Epoch 51/100\n",
      "88/88 [==============================] - 0s - loss: 0.7959 - acc: 0.8182 - val_loss: 0.9744 - val_acc: 0.9091\n",
      "Epoch 52/100\n",
      "88/88 [==============================] - 0s - loss: 0.7966 - acc: 0.8182 - val_loss: 0.9375 - val_acc: 0.9091\n",
      "Epoch 53/100\n",
      "88/88 [==============================] - 0s - loss: 0.7950 - acc: 0.8182 - val_loss: 0.9770 - val_acc: 0.9091\n",
      "Epoch 54/100\n",
      "88/88 [==============================] - 0s - loss: 0.7957 - acc: 0.8182 - val_loss: 0.9404 - val_acc: 0.9091\n",
      "Epoch 55/100\n",
      "88/88 [==============================] - 0s - loss: 0.7941 - acc: 0.8182 - val_loss: 0.9794 - val_acc: 0.9091\n",
      "Epoch 56/100\n",
      "88/88 [==============================] - 0s - loss: 0.7948 - acc: 0.8182 - val_loss: 0.9431 - val_acc: 0.9091\n",
      "Epoch 57/100\n",
      "88/88 [==============================] - 0s - loss: 0.7931 - acc: 0.8182 - val_loss: 0.9818 - val_acc: 0.9091\n",
      "Epoch 58/100\n",
      "88/88 [==============================] - 0s - loss: 0.7938 - acc: 0.8182 - val_loss: 0.9459 - val_acc: 0.9091\n",
      "Epoch 59/100\n",
      "88/88 [==============================] - 0s - loss: 0.7921 - acc: 0.8182 - val_loss: 0.9841 - val_acc: 0.9091\n",
      "Epoch 60/100\n",
      "88/88 [==============================] - 0s - loss: 0.7929 - acc: 0.8182 - val_loss: 0.9485 - val_acc: 0.9091\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s - loss: 0.7912 - acc: 0.8182 - val_loss: 0.9865 - val_acc: 0.9091\n",
      "Epoch 62/100\n",
      "88/88 [==============================] - 0s - loss: 0.7919 - acc: 0.8182 - val_loss: 0.9511 - val_acc: 0.9091\n",
      "Epoch 63/100\n",
      "88/88 [==============================] - 0s - loss: 0.7902 - acc: 0.8182 - val_loss: 0.9889 - val_acc: 0.9091\n",
      "Epoch 64/100\n",
      "88/88 [==============================] - 0s - loss: 0.7910 - acc: 0.8182 - val_loss: 0.9537 - val_acc: 0.9091\n",
      "Epoch 65/100\n",
      "88/88 [==============================] - 0s - loss: 0.7892 - acc: 0.8182 - val_loss: 0.9913 - val_acc: 0.9091\n",
      "Epoch 66/100\n",
      "88/88 [==============================] - 0s - loss: 0.7900 - acc: 0.8182 - val_loss: 0.9563 - val_acc: 0.9091\n",
      "Epoch 67/100\n",
      "88/88 [==============================] - 0s - loss: 0.7882 - acc: 0.8182 - val_loss: 0.9939 - val_acc: 0.9091\n",
      "Epoch 68/100\n",
      "88/88 [==============================] - 0s - loss: 0.7890 - acc: 0.8182 - val_loss: 0.9589 - val_acc: 0.9091\n",
      "Epoch 69/100\n",
      "88/88 [==============================] - 0s - loss: 0.7872 - acc: 0.8182 - val_loss: 0.9966 - val_acc: 0.9091\n",
      "Epoch 70/100\n",
      "88/88 [==============================] - 0s - loss: 0.7880 - acc: 0.8182 - val_loss: 0.9614 - val_acc: 0.9091\n",
      "Epoch 71/100\n",
      "88/88 [==============================] - 0s - loss: 0.7863 - acc: 0.8182 - val_loss: 0.9993 - val_acc: 0.9091\n",
      "Epoch 72/100\n",
      "88/88 [==============================] - 0s - loss: 0.7870 - acc: 0.8182 - val_loss: 0.9640 - val_acc: 0.9091\n",
      "Epoch 73/100\n",
      "88/88 [==============================] - 0s - loss: 0.7853 - acc: 0.8182 - val_loss: 1.0022 - val_acc: 0.9091\n",
      "Epoch 74/100\n",
      "88/88 [==============================] - 0s - loss: 0.7860 - acc: 0.8182 - val_loss: 0.9665 - val_acc: 0.9091\n",
      "Epoch 75/100\n",
      "88/88 [==============================] - 0s - loss: 0.7843 - acc: 0.8182 - val_loss: 1.0051 - val_acc: 0.9091\n",
      "Epoch 76/100\n",
      "88/88 [==============================] - 0s - loss: 0.7849 - acc: 0.8182 - val_loss: 0.9692 - val_acc: 0.9091\n",
      "Epoch 77/100\n",
      "88/88 [==============================] - 0s - loss: 0.7833 - acc: 0.8182 - val_loss: 1.0079 - val_acc: 0.9091\n",
      "Epoch 78/100\n",
      "88/88 [==============================] - 0s - loss: 0.7838 - acc: 0.8182 - val_loss: 0.9720 - val_acc: 0.9091\n",
      "Epoch 79/100\n",
      "88/88 [==============================] - 0s - loss: 0.7823 - acc: 0.8182 - val_loss: 1.0107 - val_acc: 0.9091\n",
      "Epoch 80/100\n",
      "88/88 [==============================] - 0s - loss: 0.7828 - acc: 0.8182 - val_loss: 0.9750 - val_acc: 0.9091\n",
      "Epoch 81/100\n",
      "88/88 [==============================] - 0s - loss: 0.7812 - acc: 0.8182 - val_loss: 1.0135 - val_acc: 0.9091\n",
      "Epoch 82/100\n",
      "88/88 [==============================] - 0s - loss: 0.7817 - acc: 0.8182 - val_loss: 0.9783 - val_acc: 0.9091\n",
      "Epoch 83/100\n",
      "88/88 [==============================] - 0s - loss: 0.7802 - acc: 0.8182 - val_loss: 1.0164 - val_acc: 0.9091\n",
      "Epoch 84/100\n",
      "88/88 [==============================] - 0s - loss: 0.7807 - acc: 0.8182 - val_loss: 0.9817 - val_acc: 0.9091\n",
      "Epoch 85/100\n",
      "88/88 [==============================] - 0s - loss: 0.7791 - acc: 0.8182 - val_loss: 1.0195 - val_acc: 0.9091\n",
      "Epoch 86/100\n",
      "88/88 [==============================] - 0s - loss: 0.7797 - acc: 0.8182 - val_loss: 0.9850 - val_acc: 0.9091\n",
      "Epoch 87/100\n",
      "88/88 [==============================] - 0s - loss: 0.7780 - acc: 0.8182 - val_loss: 1.0228 - val_acc: 0.9091\n",
      "Epoch 88/100\n",
      "88/88 [==============================] - 0s - loss: 0.7786 - acc: 0.8182 - val_loss: 0.9880 - val_acc: 0.9091\n",
      "Epoch 89/100\n",
      "88/88 [==============================] - 0s - loss: 0.7770 - acc: 0.8182 - val_loss: 1.0264 - val_acc: 0.9091\n",
      "Epoch 90/100\n",
      "88/88 [==============================] - 0s - loss: 0.7775 - acc: 0.8182 - val_loss: 0.9904 - val_acc: 0.9091\n",
      "Epoch 91/100\n",
      "88/88 [==============================] - 0s - loss: 0.7761 - acc: 0.8182 - val_loss: 1.0300 - val_acc: 0.9091\n",
      "Epoch 92/100\n",
      "88/88 [==============================] - 0s - loss: 0.7763 - acc: 0.8182 - val_loss: 0.9926 - val_acc: 0.9091\n",
      "Epoch 93/100\n",
      "88/88 [==============================] - 0s - loss: 0.7752 - acc: 0.8182 - val_loss: 1.0331 - val_acc: 0.9091\n",
      "Epoch 94/100\n",
      "88/88 [==============================] - 0s - loss: 0.7750 - acc: 0.8182 - val_loss: 0.9948 - val_acc: 0.9091\n",
      "Epoch 95/100\n",
      "88/88 [==============================] - 0s - loss: 0.7741 - acc: 0.8182 - val_loss: 1.0354 - val_acc: 0.9091\n",
      "Epoch 96/100\n",
      "88/88 [==============================] - 0s - loss: 0.7736 - acc: 0.8182 - val_loss: 0.9978 - val_acc: 0.9091\n",
      "Epoch 97/100\n",
      "88/88 [==============================] - 0s - loss: 0.7728 - acc: 0.8182 - val_loss: 1.0371 - val_acc: 0.9091\n",
      "Epoch 98/100\n",
      "88/88 [==============================] - 0s - loss: 0.7724 - acc: 0.8182 - val_loss: 1.0027 - val_acc: 0.9091\n",
      "Epoch 99/100\n",
      "88/88 [==============================] - 0s - loss: 0.7714 - acc: 0.8182 - val_loss: 1.0390 - val_acc: 0.9091\n",
      "Epoch 100/100\n",
      "88/88 [==============================] - 0s - loss: 0.7719 - acc: 0.8182 - val_loss: 1.0115 - val_acc: 0.9091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dfcaf4b908>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turn positive integers (indexes) into dense vectors of fixed size.\n",
    "bill_model = Sequential()\n",
    "bill_model.add(Embedding(input_dim=bill_input.shape[0],output_dim= 42, input_length=bill_input.shape[1]))\n",
    "# the model will take as input an integer matrix of size (batch, vocabulary_size).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, vocabulary_size, 42), where None is the batch dimension.\n",
    "\n",
    "#connect it to a dense output layer.\n",
    "bill_model.add(Flatten())\n",
    "bill_model.add(Dense(bill_output.shape[1], activation='sigmoid'))\n",
    "bill_model.summary()\n",
    "\n",
    "#training the model\n",
    "bill_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "bill_model.fit(bill_input, y=bill_output, batch_size=300, epochs=100, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Kent Beck Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 600, 42)           10668     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 25200)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 600)               15120600  \n",
      "=================================================================\n",
      "Total params: 15,131,268\n",
      "Trainable params: 15,131,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203 samples, validate on 51 samples\n",
      "Epoch 1/100\n",
      "203/203 [==============================] - 0s - loss: 6.3996 - acc: 0.0000e+00 - val_loss: 4.6277 - val_acc: 0.0196\n",
      "Epoch 2/100\n",
      "203/203 [==============================] - 0s - loss: 4.5878 - acc: 0.0197 - val_loss: 3.4758 - val_acc: 0.7843\n",
      "Epoch 3/100\n",
      "203/203 [==============================] - 0s - loss: 3.3776 - acc: 0.8030 - val_loss: 3.0783 - val_acc: 0.7843\n",
      "Epoch 4/100\n",
      "203/203 [==============================] - 0s - loss: 2.8408 - acc: 0.8030 - val_loss: 2.6213 - val_acc: 0.7843\n",
      "Epoch 5/100\n",
      "203/203 [==============================] - 0s - loss: 2.2006 - acc: 0.8030 - val_loss: 1.8992 - val_acc: 0.7843\n",
      "Epoch 6/100\n",
      "203/203 [==============================] - 0s - loss: 1.6215 - acc: 0.8030 - val_loss: 1.6480 - val_acc: 0.7843\n",
      "Epoch 7/100\n",
      "203/203 [==============================] - 0s - loss: 1.2484 - acc: 0.8030 - val_loss: 1.3847 - val_acc: 0.7843\n",
      "Epoch 8/100\n",
      "203/203 [==============================] - 0s - loss: 1.1211 - acc: 0.8030 - val_loss: 1.4431 - val_acc: 0.7843\n",
      "Epoch 9/100\n",
      "203/203 [==============================] - 0s - loss: 1.0916 - acc: 0.8030 - val_loss: 1.4006 - val_acc: 0.7843\n",
      "Epoch 10/100\n",
      "203/203 [==============================] - 0s - loss: 1.0795 - acc: 0.8030 - val_loss: 1.4286 - val_acc: 0.7843\n",
      "Epoch 11/100\n",
      "203/203 [==============================] - 0s - loss: 1.0726 - acc: 0.8030 - val_loss: 1.4003 - val_acc: 0.7843\n",
      "Epoch 12/100\n",
      "203/203 [==============================] - 0s - loss: 1.0680 - acc: 0.8030 - val_loss: 1.4315 - val_acc: 0.7843\n",
      "Epoch 13/100\n",
      "203/203 [==============================] - 0s - loss: 1.0648 - acc: 0.8030 - val_loss: 1.3949 - val_acc: 0.7843\n",
      "Epoch 14/100\n",
      "203/203 [==============================] - 0s - loss: 1.0628 - acc: 0.8030 - val_loss: 1.4494 - val_acc: 0.7843\n",
      "Epoch 15/100\n",
      "203/203 [==============================] - 0s - loss: 1.0619 - acc: 0.8030 - val_loss: 1.3750 - val_acc: 0.7843\n",
      "Epoch 16/100\n",
      "203/203 [==============================] - 0s - loss: 1.0644 - acc: 0.8030 - val_loss: 1.5052 - val_acc: 0.7843\n",
      "Epoch 17/100\n",
      "203/203 [==============================] - 0s - loss: 1.0692 - acc: 0.8030 - val_loss: 1.3359 - val_acc: 0.7843\n",
      "Epoch 18/100\n",
      "203/203 [==============================] - 0s - loss: 1.0870 - acc: 0.8030 - val_loss: 1.6052 - val_acc: 0.7843\n",
      "Epoch 19/100\n",
      "203/203 [==============================] - 0s - loss: 1.0930 - acc: 0.8030 - val_loss: 1.3375 - val_acc: 0.7843\n",
      "Epoch 20/100\n",
      "203/203 [==============================] - 0s - loss: 1.0924 - acc: 0.8030 - val_loss: 1.5645 - val_acc: 0.7843\n",
      "Epoch 21/100\n",
      "203/203 [==============================] - 0s - loss: 1.0803 - acc: 0.8030 - val_loss: 1.3609 - val_acc: 0.7843\n",
      "Epoch 22/100\n",
      "203/203 [==============================] - 0s - loss: 1.0664 - acc: 0.8030 - val_loss: 1.5243 - val_acc: 0.7843\n",
      "Epoch 23/100\n",
      "203/203 [==============================] - 0s - loss: 1.0651 - acc: 0.8030 - val_loss: 1.3649 - val_acc: 0.7843\n",
      "Epoch 24/100\n",
      "203/203 [==============================] - 0s - loss: 1.0641 - acc: 0.8030 - val_loss: 1.5277 - val_acc: 0.7843\n",
      "Epoch 25/100\n",
      "203/203 [==============================] - 0s - loss: 1.0650 - acc: 0.8030 - val_loss: 1.3701 - val_acc: 0.7843\n",
      "Epoch 26/100\n",
      "203/203 [==============================] - 0s - loss: 1.0675 - acc: 0.8030 - val_loss: 1.5273 - val_acc: 0.7843\n",
      "Epoch 27/100\n",
      "203/203 [==============================] - 0s - loss: 1.0715 - acc: 0.8030 - val_loss: 1.3892 - val_acc: 0.7843\n",
      "Epoch 28/100\n",
      "203/203 [==============================] - 0s - loss: 1.0778 - acc: 0.8030 - val_loss: 1.5181 - val_acc: 0.7843\n",
      "Epoch 29/100\n",
      "203/203 [==============================] - 0s - loss: 1.0818 - acc: 0.8030 - val_loss: 1.3674 - val_acc: 0.7843\n",
      "Epoch 30/100\n",
      "203/203 [==============================] - 0s - loss: 1.0672 - acc: 0.8030 - val_loss: 1.5411 - val_acc: 0.7843\n",
      "Epoch 31/100\n",
      "203/203 [==============================] - 0s - loss: 1.0649 - acc: 0.8030 - val_loss: 1.3736 - val_acc: 0.7843\n",
      "Epoch 32/100\n",
      "203/203 [==============================] - 0s - loss: 1.0623 - acc: 0.8030 - val_loss: 1.5382 - val_acc: 0.7843\n",
      "Epoch 33/100\n",
      "203/203 [==============================] - 0s - loss: 1.0628 - acc: 0.8030 - val_loss: 1.3786 - val_acc: 0.7843\n",
      "Epoch 34/100\n",
      "203/203 [==============================] - 0s - loss: 1.0610 - acc: 0.8030 - val_loss: 1.5353 - val_acc: 0.7843\n",
      "Epoch 35/100\n",
      "203/203 [==============================] - 0s - loss: 1.0624 - acc: 0.8030 - val_loss: 1.3839 - val_acc: 0.7843\n",
      "Epoch 36/100\n",
      "203/203 [==============================] - 0s - loss: 1.0619 - acc: 0.8030 - val_loss: 1.5329 - val_acc: 0.7843\n",
      "Epoch 37/100\n",
      "203/203 [==============================] - 0s - loss: 1.0639 - acc: 0.8030 - val_loss: 1.3863 - val_acc: 0.7843\n",
      "Epoch 38/100\n",
      "203/203 [==============================] - 0s - loss: 1.0623 - acc: 0.8030 - val_loss: 1.5342 - val_acc: 0.7843\n",
      "Epoch 39/100\n",
      "203/203 [==============================] - 0s - loss: 1.0632 - acc: 0.8030 - val_loss: 1.3853 - val_acc: 0.7843\n",
      "Epoch 40/100\n",
      "203/203 [==============================] - 0s - loss: 1.0600 - acc: 0.8030 - val_loss: 1.5378 - val_acc: 0.7843\n",
      "Epoch 41/100\n",
      "203/203 [==============================] - 0s - loss: 1.0601 - acc: 0.8030 - val_loss: 1.3863 - val_acc: 0.7843\n",
      "Epoch 42/100\n",
      "203/203 [==============================] - 0s - loss: 1.0573 - acc: 0.8030 - val_loss: 1.5398 - val_acc: 0.7843\n",
      "Epoch 43/100\n",
      "203/203 [==============================] - 0s - loss: 1.0578 - acc: 0.8030 - val_loss: 1.3877 - val_acc: 0.7843\n",
      "Epoch 44/100\n",
      "203/203 [==============================] - 0s - loss: 1.0553 - acc: 0.8030 - val_loss: 1.5405 - val_acc: 0.7843\n",
      "Epoch 45/100\n",
      "203/203 [==============================] - 0s - loss: 1.0561 - acc: 0.8030 - val_loss: 1.3897 - val_acc: 0.7843\n",
      "Epoch 46/100\n",
      "203/203 [==============================] - 0s - loss: 1.0539 - acc: 0.8030 - val_loss: 1.5413 - val_acc: 0.7843\n",
      "Epoch 47/100\n",
      "203/203 [==============================] - 0s - loss: 1.0551 - acc: 0.8030 - val_loss: 1.3911 - val_acc: 0.7843\n",
      "Epoch 48/100\n",
      "203/203 [==============================] - 0s - loss: 1.0527 - acc: 0.8030 - val_loss: 1.5419 - val_acc: 0.7843\n",
      "Epoch 49/100\n",
      "203/203 [==============================] - 0s - loss: 1.0538 - acc: 0.8030 - val_loss: 1.3925 - val_acc: 0.7843\n",
      "Epoch 50/100\n",
      "203/203 [==============================] - 0s - loss: 1.0513 - acc: 0.8030 - val_loss: 1.5429 - val_acc: 0.7843\n",
      "Epoch 51/100\n",
      "203/203 [==============================] - 0s - loss: 1.0524 - acc: 0.8030 - val_loss: 1.3935 - val_acc: 0.7843\n",
      "Epoch 52/100\n",
      "203/203 [==============================] - 0s - loss: 1.0496 - acc: 0.8030 - val_loss: 1.5437 - val_acc: 0.7843\n",
      "Epoch 53/100\n",
      "203/203 [==============================] - 0s - loss: 1.0506 - acc: 0.8030 - val_loss: 1.3949 - val_acc: 0.7843\n",
      "Epoch 54/100\n",
      "203/203 [==============================] - 0s - loss: 1.0476 - acc: 0.8030 - val_loss: 1.5448 - val_acc: 0.7843\n",
      "Epoch 55/100\n",
      "203/203 [==============================] - 0s - loss: 1.0489 - acc: 0.8030 - val_loss: 1.3960 - val_acc: 0.7843\n",
      "Epoch 56/100\n",
      "203/203 [==============================] - 0s - loss: 1.0458 - acc: 0.8030 - val_loss: 1.5458 - val_acc: 0.7843\n",
      "Epoch 57/100\n",
      "203/203 [==============================] - 0s - loss: 1.0472 - acc: 0.8030 - val_loss: 1.3976 - val_acc: 0.7843\n",
      "Epoch 58/100\n",
      "203/203 [==============================] - 0s - loss: 1.0441 - acc: 0.8030 - val_loss: 1.5468 - val_acc: 0.7843\n",
      "Epoch 59/100\n",
      "203/203 [==============================] - 0s - loss: 1.0457 - acc: 0.8030 - val_loss: 1.3985 - val_acc: 0.7843\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 0s - loss: 1.0424 - acc: 0.8030 - val_loss: 1.5472 - val_acc: 0.7843\n",
      "Epoch 61/100\n",
      "203/203 [==============================] - 0s - loss: 1.0439 - acc: 0.8030 - val_loss: 1.3998 - val_acc: 0.7843\n",
      "Epoch 62/100\n",
      "203/203 [==============================] - 0s - loss: 1.0408 - acc: 0.8030 - val_loss: 1.5473 - val_acc: 0.7843\n",
      "Epoch 63/100\n",
      "203/203 [==============================] - 0s - loss: 1.0421 - acc: 0.8030 - val_loss: 1.4006 - val_acc: 0.7843\n",
      "Epoch 64/100\n",
      "203/203 [==============================] - 0s - loss: 1.0392 - acc: 0.8030 - val_loss: 1.5468 - val_acc: 0.7843\n",
      "Epoch 65/100\n",
      "203/203 [==============================] - 0s - loss: 1.0400 - acc: 0.8030 - val_loss: 1.4017 - val_acc: 0.7843\n",
      "Epoch 66/100\n",
      "203/203 [==============================] - 0s - loss: 1.0375 - acc: 0.8030 - val_loss: 1.5465 - val_acc: 0.7843\n",
      "Epoch 67/100\n",
      "203/203 [==============================] - 0s - loss: 1.0380 - acc: 0.8030 - val_loss: 1.4026 - val_acc: 0.7843\n",
      "Epoch 68/100\n",
      "203/203 [==============================] - 0s - loss: 1.0357 - acc: 0.8030 - val_loss: 1.5468 - val_acc: 0.7843\n",
      "Epoch 69/100\n",
      "203/203 [==============================] - 0s - loss: 1.0360 - acc: 0.8030 - val_loss: 1.4042 - val_acc: 0.7843\n",
      "Epoch 70/100\n",
      "203/203 [==============================] - 0s - loss: 1.0337 - acc: 0.8030 - val_loss: 1.5486 - val_acc: 0.7843\n",
      "Epoch 71/100\n",
      "203/203 [==============================] - 0s - loss: 1.0343 - acc: 0.8030 - val_loss: 1.4057 - val_acc: 0.7843\n",
      "Epoch 72/100\n",
      "203/203 [==============================] - 0s - loss: 1.0315 - acc: 0.8030 - val_loss: 1.5519 - val_acc: 0.7843\n",
      "Epoch 73/100\n",
      "203/203 [==============================] - 0s - loss: 1.0329 - acc: 0.8030 - val_loss: 1.4085 - val_acc: 0.7843\n",
      "Epoch 74/100\n",
      "203/203 [==============================] - 0s - loss: 1.0293 - acc: 0.8030 - val_loss: 1.5546 - val_acc: 0.7843\n",
      "Epoch 75/100\n",
      "203/203 [==============================] - 0s - loss: 1.0315 - acc: 0.8030 - val_loss: 1.4114 - val_acc: 0.7843\n",
      "Epoch 76/100\n",
      "203/203 [==============================] - 0s - loss: 1.0268 - acc: 0.8030 - val_loss: 1.5539 - val_acc: 0.7843\n",
      "Epoch 77/100\n",
      "203/203 [==============================] - 0s - loss: 1.0291 - acc: 0.8030 - val_loss: 1.4134 - val_acc: 0.7843\n",
      "Epoch 78/100\n",
      "203/203 [==============================] - 0s - loss: 1.0247 - acc: 0.8030 - val_loss: 1.5521 - val_acc: 0.7843\n",
      "Epoch 79/100\n",
      "203/203 [==============================] - 0s - loss: 1.0266 - acc: 0.8030 - val_loss: 1.4118 - val_acc: 0.7843\n",
      "Epoch 80/100\n",
      "203/203 [==============================] - 0s - loss: 1.0244 - acc: 0.8030 - val_loss: 1.5503 - val_acc: 0.7843\n",
      "Epoch 81/100\n",
      "203/203 [==============================] - 0s - loss: 1.0251 - acc: 0.8030 - val_loss: 1.4113 - val_acc: 0.7843\n",
      "Epoch 82/100\n",
      "203/203 [==============================] - 0s - loss: 1.0270 - acc: 0.8030 - val_loss: 1.5337 - val_acc: 0.7843\n",
      "Epoch 83/100\n",
      "203/203 [==============================] - 0s - loss: 1.0216 - acc: 0.8030 - val_loss: 1.4217 - val_acc: 0.7843\n",
      "Epoch 84/100\n",
      "203/203 [==============================] - 0s - loss: 1.0251 - acc: 0.8030 - val_loss: 1.5014 - val_acc: 0.7843\n",
      "Epoch 85/100\n",
      "203/203 [==============================] - 0s - loss: 1.0153 - acc: 0.8030 - val_loss: 1.4507 - val_acc: 0.7843\n",
      "Epoch 86/100\n",
      "203/203 [==============================] - 0s - loss: 1.0194 - acc: 0.8030 - val_loss: 1.4654 - val_acc: 0.7843\n",
      "Epoch 87/100\n",
      "203/203 [==============================] - 0s - loss: 1.0130 - acc: 0.8030 - val_loss: 1.5176 - val_acc: 0.7843\n",
      "Epoch 88/100\n",
      "203/203 [==============================] - 0s - loss: 1.0198 - acc: 0.8030 - val_loss: 1.4346 - val_acc: 0.7843\n",
      "Epoch 89/100\n",
      "203/203 [==============================] - 0s - loss: 1.0191 - acc: 0.8030 - val_loss: 1.6015 - val_acc: 0.7843\n",
      "Epoch 90/100\n",
      "203/203 [==============================] - 0s - loss: 1.0308 - acc: 0.8030 - val_loss: 1.4635 - val_acc: 0.7843\n",
      "Epoch 91/100\n",
      "203/203 [==============================] - 0s - loss: 1.0201 - acc: 0.8030 - val_loss: 1.5354 - val_acc: 0.7843\n",
      "Epoch 92/100\n",
      "203/203 [==============================] - 0s - loss: 1.0271 - acc: 0.8030 - val_loss: 1.4464 - val_acc: 0.7843\n",
      "Epoch 93/100\n",
      "203/203 [==============================] - 0s - loss: 1.0168 - acc: 0.8030 - val_loss: 1.5343 - val_acc: 0.7843\n",
      "Epoch 94/100\n",
      "203/203 [==============================] - 0s - loss: 1.0132 - acc: 0.8030 - val_loss: 1.4520 - val_acc: 0.7843\n",
      "Epoch 95/100\n",
      "203/203 [==============================] - 0s - loss: 1.0083 - acc: 0.8030 - val_loss: 1.5248 - val_acc: 0.7843\n",
      "Epoch 96/100\n",
      "203/203 [==============================] - 0s - loss: 1.0072 - acc: 0.8030 - val_loss: 1.4577 - val_acc: 0.7843\n",
      "Epoch 97/100\n",
      "203/203 [==============================] - 0s - loss: 1.0047 - acc: 0.8030 - val_loss: 1.5257 - val_acc: 0.7843\n",
      "Epoch 98/100\n",
      "203/203 [==============================] - 0s - loss: 1.0060 - acc: 0.8030 - val_loss: 1.4619 - val_acc: 0.7843\n",
      "Epoch 99/100\n",
      "203/203 [==============================] - 0s - loss: 1.0041 - acc: 0.8030 - val_loss: 1.5318 - val_acc: 0.7843\n",
      "Epoch 100/100\n",
      "203/203 [==============================] - 0s - loss: 1.0073 - acc: 0.8030 - val_loss: 1.4645 - val_acc: 0.7843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1df807644e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turn positive integers (indexes) into dense vectors of fixed size.\n",
    "kent_model = Sequential()\n",
    "kent_model.add(Embedding(input_dim=kent_input.shape[0],output_dim= 42, input_length=kent_input.shape[1]))\n",
    "# the model will take as input an integer matrix of size (batch, vocabulary_size).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, vocabulary_size, 42), where None is the batch dimension.\n",
    "\n",
    "#connect it to a dense output layer.\n",
    "kent_model.add(Flatten())\n",
    "kent_model.add(Dense(kent_output.shape[1], activation='sigmoid'))\n",
    "kent_model.summary()\n",
    "\n",
    "#training the model\n",
    "kent_model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "kent_model.fit(kent_input, y=kent_output, batch_size=300, epochs=100, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Chcek The Models</h2> <br />\n",
    "We will define a function that accepts a word, convert it to its one-hot representation, predict the following word using the trained model, and finally convert the predicted one-hot result into a text and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(text,token,model,fullmtx,fullText):\n",
    "    tmp = text_to_word_sequence(text, lower=False, split=\" \")\n",
    "    tmp = token.texts_to_matrix(tmp, mode='binary')\n",
    "    p = model.predict(tmp)\n",
    "    top10 = p.argsort() [0][-10:]\n",
    "    bestMatch = np.random.choice(top10,1)[0]\n",
    "    next_idx = np.min(np.where(fullmtx[:,bestMatch]>0))\n",
    "    return fullText[next_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Function for creating tweets</h3> <br />\n",
    "W'll define a function that generates 30% tweets from the data for each celebruty and also the function checks that each tweet is no more than 140 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def creatingTweets(epochs,token,model,fullMtx,fullText):\n",
    "    output_text = \"\"\n",
    "    for i in range(0,epochs-1):\n",
    "        tweet =\"\"\n",
    "        word = get_next('TWEETSTART',token,model,fullMtx,fullText)\n",
    "        while(word != 'TWEETEND' and len(tweet) <= (140 - (len(word) + 1))):\n",
    "            if (word != 'TWEETSTART'):\n",
    "                tweet += ' ' + word\n",
    "            word = get_next(word,token,model,fullMtx,fullText)\n",
    "        output_text += tweet + '. '\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets generate tweets for each celebrity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' never forget wisconsin.  made fake despit news job great UNKNOWNTOKEN great agenda america hunt news time great new great UNKNOWNTOKEN witch hunt great despit great.  made. .  never job high phoni great.  rt UNKNOWNTOKEN witch.  despit phoni stori russian great number hunt mani news great.  thank wisconsin hunt great news made news great number russian.  rt.  fake despit phoni great UNKNOWNTOKEN friend happi great news fake great state.  UNKNOWNTOKEN great news report.  congratul hunt hunt UNKNOWNTOKEN.  thank news time russian UNKNOWNTOKEN UNKNOWNTOKEN UNKNOWNTOKEN friend russian news thank phoni collus.  never. '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_new_tweets = creatingTweets(15,trump_token,trump_model,trump_text_mtx,trump_seq)\n",
    "kim_new_tweets = creatingTweets(15,kim_token,kim_model,kim_text_mtx,kim_seq)\n",
    "katy_new_tweets = creatingTweets(15,katy_token,katy_model,katy_text_mtx,katy_seq)\n",
    "#bill_new_tweets = creatingTweets(10,bill_token,bill_model,bill_text_mtx,bill_seq)\n",
    "kent_new_tweets = creatingTweets(10,kent_token,kent_model,kent_text_mtx,kent_seq)\n",
    "trump_new_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now let's convert our new tweets to data frame</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realResult</th>\n",
       "      <th>screenName</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>remark happi peopl rt UNKNOWNTOKEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>great news job great state wisconsin great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>passag amp go passag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>happi great news amp UNKNOWNTOKEN great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>never UNKNOWNTOKEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>rt great agenda UNKNOWNTOKEN UNKNOWNTOKEN wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWNTOKEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>happi stori fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td></td>\n",
       "      <td>congratul stori peopl great news amp great UN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        realResult screenName  \\\n",
       "0  realDonaldTrump              \n",
       "1  realDonaldTrump              \n",
       "2  realDonaldTrump              \n",
       "3  realDonaldTrump              \n",
       "4  realDonaldTrump              \n",
       "5  realDonaldTrump              \n",
       "6  realDonaldTrump              \n",
       "7  realDonaldTrump              \n",
       "8  realDonaldTrump              \n",
       "9  realDonaldTrump              \n",
       "\n",
       "                                                text  \n",
       "0                 remark happi peopl rt UNKNOWNTOKEN  \n",
       "1         great news job great state wisconsin great  \n",
       "2                               passag amp go passag  \n",
       "3            happi great news amp UNKNOWNTOKEN great  \n",
       "4                                 never UNKNOWNTOKEN  \n",
       "5   rt great agenda UNKNOWNTOKEN UNKNOWNTOKEN wit...  \n",
       "6                                       UNKNOWNTOKEN  \n",
       "7                                              never  \n",
       "8                                   happi stori fake  \n",
       "9   congratul stori peopl great news amp great UN...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_tweets_to_dataframe(text):\n",
    "    columns = \"text\"\n",
    "    tweets = text.split('. ')\n",
    "    df_text = pd.DataFrame(columns=columns.split(' '),data=[t for t in tweets])\n",
    "    df_screen_name = pd.DataFrame(columns=['screenName','realResult'])\n",
    "    df_output = pd.concat([df_text, df_screen_name])\n",
    "    return df_output[:-1]\n",
    "\n",
    "#creates data frame to each celebrity\n",
    "trump_df = convert_tweets_to_dataframe(trump_new_tweets)\n",
    "trump_df['realResult'] = 'realDonaldTrump'\n",
    "kim_df = convert_tweets_to_dataframe(kim_new_tweets)\n",
    "kim_df['realResult'] = 'KimKardashian'\n",
    "katy_df = convert_tweets_to_dataframe(katy_new_tweets)\n",
    "katy_df['realResult'] = 'katyperry'\n",
    "kent_df = convert_tweets_to_dataframe(kent_new_tweets)\n",
    "kent_df['realResult'] = 'KentBeck'\n",
    "\n",
    "new_tweets_df = pd.concat([trump_df,kim_df,katy_df,kent_df],ignore_index=True)\n",
    "new_tweets_df = new_tweets_df.replace(np.nan, '', regex=True)\n",
    "new_tweets_df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Final Project Part B</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use out classification model from part B to predict the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Cleaning:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through and clean all of the training set at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train_tweets = train['text'].apply(lambda x: clean_tweets(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>BOW - Bag of words</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "(179, 870)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_tweets)\n",
    "# convert the result to an array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary after the Bag of Words model is trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abl', 'abus', 'account', 'accur', 'acknowledg', 'act', 'ad', 'african', 'ag', 'age', 'agenda', 'agileklzkitten', 'ago', 'ahead', 'aint', 'air', 'akutienamekim', 'album', 'alechuerta', 'alli', 'almostanart', 'alreadi', 'also', 'aluminum', 'alway', 'amaz', 'america', 'americafirst', 'american', 'amp', 'amsterdam', 'andynishan', 'angel', 'annual', 'anoth', 'answer', 'anyon', 'anyth', 'apolog', 'app', 'approv', 'appétit', 'armi', 'around', 'arriv', 'art', 'ask', 'assess', 'asshol', 'aussie_kardash', 'autograph', 'avail', 'awkwardaya', 'babe', 'babi', 'back', 'bad', 'badli', 'ball', 'ban', 'beauti', 'bed', 'begin', 'behind', 'believ', 'best', 'better', 'big', 'bigbabyjonathan', 'biggi', 'bigsean', 'bike', 'bill', 'bin', 'bird', 'birthday', 'bleach', 'blend', 'bling', 'bloomberg', 'bon', 'bonu', 'book', 'box', 'br', 'braveri', 'bring', 'brush', 'budget', 'buffalo', 'build', 'busi', 'ca', 'cabinet', 'call', 'calvinharri', 'came', 'campaign', 'cant', 'capitolrecord', 'care', 'carlythekatycat', 'case', 'cat', 'catch', 'ceremoni', 'ch', 'chain', 'chanc', 'chariti', 'charlott', 'check', 'chicgeek', 'chill', 'chuck', 'circuit', 'citi', 'clairecmc', 'clear', 'clemsonfb', 'clinton', 'close', 'coal', 'coalit', 'code', 'collus', 'come', 'comey', 'commanderinchief', 'comment', 'commit', 'committe', 'compani', 'compel', 'complement', 'comput', 'con', 'confid', 'conflict', 'congratul', 'contour', 'control', 'controversi', 'convers', 'cool', 'corrynmb', 'costcos', 'could', 'couldnt', 'countri', 'cowardli', 'creat', 'creme', 'crook', 'crème', 'créme', 'cuba', 'cubanamerican', 'cuff', 'cup', 'current', 'cut', 'dad', 'daddi', 'daili', 'danc', 'danger', 'dark', 'dash', 'dashboutiqu', 'daughter', 'day', 'dead', 'deal', 'death', 'decis', 'dedic', 'deep', 'defund', 'deireyperri', 'delet', 'dem', 'democrat', 'deptvetaffair', 'despit', 'destroy', 'develop', 'devil', 'diff', 'differ', 'difficult', 'dinner', 'director', 'dirti', 'discuss', 'distract', 'doesnt', 'doll', 'done', 'dont', 'door', 'dow', 'down', 'draft', 'draw', 'dream', 'drill', 'drink', 'drop', 'drove', 'dualend', 'earli', 'easier', 'econom', 'economi', 'edit', 'effort', 'elect', 'elimin', 'els', 'em', 'email', 'emot', 'end', 'endthanky', 'energi', 'enforc', 'engin', 'enjoy', 'enthusiasm', 'era', 'ever', 'everi', 'everyth', 'evid', 'evrenperri', 'excit', 'exclus', 'execut', 'experi', 'experienc', 'facebook', 'fail', 'faithandfreedom', 'fake', 'fakenew', 'fam', 'famili', 'fantast', 'far', 'father', 'fathersday', 'fault', 'favian_flor', 'fbi', 'fearless', 'feel', 'femal', 'fidget', 'fight', 'fighter', 'figueroa', 'film', 'final', 'finger', 'fire', 'first', 'fitzgerald', 'five', 'forget', 'found', 'four', 'foxandfriend', 'free', 'friday', 'friend', 'full', 'fulli', 'fun', 'futur', 'gave', 'geraldorivera', 'get', 'gift', 'girl', 'give', 'go', 'god', 'good', 'gorgeou', 'got', 'govern', 'govt', 'govwalk', 'grand', 'grate', 'great', 'greatest', 'group', 'growin', 'guess', 'gut', 'hammer', 'hanniti', 'happi', 'happycustom', 'harass', 'hard', 'hat', 'hate', 'he', 'head', 'health', 'healthcar', 'hear', 'held', 'help', 'here', 'hey', 'high', 'higher', 'highlight', 'hillari', 'histori', 'hit', 'home', 'honest', 'honor', 'hope', 'horrif', 'hospit', 'host', 'hour', 'hous', 'how', 'hq', 'hr', 'hrhjesu', 'http', 'humbl', 'hunt', 'husband', 'idkisaac', 'iff', 'ignor', 'illeg', 'im', 'imagin', 'impeach', 'import', 'inadvert', 'includ', 'incorrect', 'incred', 'infrastructur', 'infrastructureweek', 'injur', 'inspir', 'instead', 'intent', 'interest', 'interweb', 'intomeyouse', 'inventori', 'investig', 'iron', 'ivanka', 'ivankatrump', 'ive', 'jame', 'japanes', 'jessica', 'job', 'johanneslink', 'johncena', 'johnlordperri', 'join', 'jointli', 'jordynrtorr', 'june', 'justic', 'justifi', 'kansa', 'kardashian', 'kati', 'katycat', 'katycatdevin', 'katycatteagan', 'katyperri', 'kaylaschmitz', 'keek', 'keep', 'kenya', 'key', 'khloekardashian', 'kick', 'kill', 'kimkanyekimyefc', 'kimkardashian', 'kimkourtkhloek', 'kimoji', 'kimsingtheatr', 'kit', 'kkwbeauti', 'kkwstan', 'kourtneykardash', 'kpcollect', 'kpwww', 'krisjenn', 'krystan_brook', 'kuw', 'kuwthewest', 'kuwtk', 'kyliesaturn', 'laforet', 'last', 'laugh', 'launch', 'law', 'leader', 'leak', 'leav', 'led', 'left', 'legisl', 'less', 'let', 'level', 'liber', 'light', 'like', 'line', 'listen', 'lit', 'littl', 'live', 'lo', 'long', 'look', 'lost', 'lot', 'louisiana', 'love', 'lovekati', 'loyal', 'lynch', 'lynchfound', 'made', 'maga', 'magic', 'make', 'makeupbymario', 'man', 'mani', 'marick', 'massiv', 'matt', 'mean', 'media', 'medium', 'meet', 'melisfoulk', 'memorandum', 'merch', 'merchandis', 'merci', 'messag', 'met', 'miami', 'michaelantni', 'might', 'mile', 'million', 'mine', 'minim', 'minut', 'misogynist', 'miss', 'money', 'monologu', 'month', 'moon', 'moral', 'morn', 'move', 'mpesa', 'ms', 'msm', 'mstodd', 'much', 'music', 'nadayar', 'name', 'narbehkardash', 'nasdaq', 'nation', 'nationalchampion', 'nebraska', 'need', 'net', 'never', 'new', 'news', 'newtgingrich', 'nice', 'nitsanw', 'nobodi', 'nondeal', 'nope', 'north', 'noth', 'novemb', 'nugget', 'number', 'ny', 'obamacar', 'obsess', 'obstruct', 'obstructionist', 'offer', 'old', 'omaha', 'one', 'open', 'opportun', 'option', 'order', 'os', 'ottowarmbi', 'pablo', 'pack', 'paralyz', 'pari', 'pass', 'passag', 'past', 'patrick_mc', 'patriot', 'paulboy', 'pennsylvania', 'peopl', 'perfect', 'perhap', 'person', 'perspect', 'pharrel', 'phone', 'phoni', 'pic', 'pick', 'pin', 'place', 'plain', 'planetkhloek', 'play', 'pleas', 'pledg', 'pm', 'polaroid', 'polici', 'polio', 'polit', 'poll', 'pom', 'pop', 'popit', 'popup', 'possibl', 'power', 'pp', 'ppl', 'pray', 'prayer', 'predict', 'present', 'presid', 'presidenti', 'preval', 'preview', 'price', 'pride', 'privat', 'problem', 'proclam', 'programm', 'progress', 'project', 'promis', 'proof', 'propercloth', 'prosper', 'protect', 'proud', 'provid', 'pst', 'pulsenightclub', 'puppi', 'purpos', 'purposesgav', 'put', 'quebec', 'queen', 'rajpanjabi', 'rapid', 'rasmussen', 'rather', 'ratingthat', 'reach', 'readi', 'real', 'realdonaldtrump', 'realli', 'rebuild', 'receiv', 'record', 'recov', 'refactor', 'reganneliz', 'regul', 'remark', 'rep', 'report', 'resist', 'restor', 'richardgrenel', 'ride', 'right', 'rollercoast', 'rotari', 'rt', 'rule', 'russia', 'russian', 'sacr', 'sacrific', 'sad', 'safe', 'sailor', 'sale', 'saleandro', 'sammobrien', 'saturday', 'save', 'say', 'sc', 'scalis', 'scare', 'scene', 'schedul', 'scjustic', 'seanhann', 'search', 'season', 'secretli', 'sector', 'secur', 'see', 'seen', 'sehurlburt', 'selfdoubt', 'senat', 'send', 'serv', 'set', 'sexual', 'shade', 'shannonflynn', 'shape', 'shark', 'she', 'shimmer', 'shirt', 'shitti', 'shoot', 'shop', 'short', 'show', 'sign', 'sinc', 'singl', 'sip', 'sister', 'skin', 'sleev', 'slept', 'smaller', 'smalltalk', 'social', 'soldier', 'solut', 'someon', 'soniaaperri', 'soon', 'soplain', 'sorri', 'sourc', 'space', 'spent', 'spill', 'spinner', 'spiral', 'spong', 'spring', 'st', 'stand', 'standwithpp', 'start', 'state', 'statement', 'stay', 'steel', 'step', 'steve', 'stick', 'still', 'stop', 'store', 'stori', 'strengthen', 'stronger', 'success', 'suffer', 'summar', 'summit', 'super', 'support', 'sure', 'surpris', 'swishswish', 'tag', 'take', 'talk', 'target', 'tax', 'tea', 'team', 'teamkp', 'tend', 'testimoni', 'tfw', 'th', 'thacker', 'thank', 'that', 'theboiledpnut', 'thing', 'thought', 'thr', 'thread', 'ticket', 'tidi', 'tiger', 'til', 'tillerson', 'time', 'tix', 'today', 'todayshow', 'tokyo', 'told', 'toler', 'tomorro', 'tomorrow', 'tone', 'tonight', 'took', 'tool', 'total', 'tough', 'tour', 'toward', 'town', 'track', 'transcript', 'transform', 'travel', 'travisk', 'trend', 'tri', 'triumph', 'true', 'truli', 'trump', 'tuckercarlson', 'tuesday', 'tune', 'turn', 'turntkendali', 'turntprincess_', 'tweendream', 'tweet', 'twitter', 'two', 'ty', 'uae', 'uber', 'unemploy', 'unit', 'unreport', 'up', 'us', 'usa', 'use', 'uss', 'valor', 'varunjuic', 'versatil', 'version', 'veteran', 'via', 'vibe', 'victim', 'video', 'visionari', 'visit', 'viva_la_eva_', 'voic', 'vulner', 'wag', 'wait', 'wake', 'walk', 'wanna', 'want', 'warn', 'warrenbuffett', 'wasnt', 'watch', 'way', 'wctc', 'wed', 'welcom', 'well', 'welovekuwtk', 'went', 'weve', 'what', 'whistleblow', 'whitehous', 'who', 'win', 'wisconsin', 'wish', 'wit', 'witch', 'without', 'witnessthetour', 'wonder', 'wonderwoman', 'woof', 'work', 'worker', 'workforceweek', 'workim', 'world', 'worri', 'worship', 'would', 'wrap', 'write', 'writeup', 'wrong', 'x_lauren_x', 'ye', 'year', 'yesterday', 'yet', 'yittledez', 'your', 'zero']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now We will check the Results with our classification model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73170731707317072"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#split to train & test\n",
    "msk = np.random.rand(len(train)) < 0.8\n",
    "train_x = train_data_features[msk]\n",
    "test_x = train_data_features[~msk]\n",
    "train_y = train.loc[msk,\"screenName\"]\n",
    "test_y = train.loc[~msk,\"screenName\"]\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_x, train_y )\n",
    "# Evaluate accuracy best on the test set\n",
    "res = forest.score(test_x,test_y)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Check Our New Tweets With Our Trained Random Forest Classifier</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We creating a new vectorizer with the vocabulary of our previous one and predict who wrote each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['KimKardashian', 'realDonaldTrump', 'realDonaldTrump',\n",
       "       'realDonaldTrump', 'KimKardashian', 'realDonaldTrump',\n",
       "       'KimKardashian', 'KimKardashian', 'KimKardashian',\n",
       "       'realDonaldTrump', 'realDonaldTrump', 'KimKardashian',\n",
       "       'realDonaldTrump', 'realDonaldTrump', 'KimKardashian',\n",
       "       'KimKardashian', 'KimKardashian', 'KimKardashian', 'KimKardashian',\n",
       "       'KimKardashian', 'KimKardashian', 'KimKardashian', 'KimKardashian',\n",
       "       'KimKardashian', 'KimKardashian', 'KimKardashian', 'KimKardashian',\n",
       "       'KimKardashian', 'katyperry', 'katyperry', 'katyperry', 'katyperry',\n",
       "       'katyperry', 'katyperry', 'katyperry', 'katyperry', 'katyperry',\n",
       "       'katyperry', 'katyperry', 'katyperry', 'katyperry', 'katyperry',\n",
       "       'KentBeck', 'KentBeck', 'KentBeck', 'KentBeck', 'KentBeck',\n",
       "       'KentBeck', 'KentBeck', 'KentBeck', 'KentBeck'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "vectorizer_new = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000,\n",
    "                             vocabulary = vectorizer.vocabulary_) \n",
    "\n",
    "train_data_features_new = vectorizer_new.fit_transform(new_tweets_df['text'])\n",
    "# convert the result to an array\n",
    "train_data_features_new = train_data_features_new.toarray()\n",
    "\n",
    "predicted_results = forest.predict(train_data_features_new)\n",
    "predicted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the accuracy of the predicted results according to the real results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823529411764706\n"
     ]
    }
   ],
   "source": [
    "new_tweets_df['screenName'] = predicted_results\n",
    "diff_df = new_tweets_df[new_tweets_df['screenName'] != new_tweets_df['realResult']]\n",
    "accuracy = abs(new_tweets_df.shape[0] - diff_df.shape[0]) / new_tweets_df.shape[0]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create the confusion matrix of our mistakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\tTrump\tKim \tKaty\tKent\n",
      "Trump\t   9\t   0\t   0\t   0\n",
      "Kim \t   0\t  14\t   0\t   0\n",
      "Katy\t   0\t   0\t  14\t   0\n",
      "Kent\t   0\t   6\t   0\t   8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "c_m = confusion_matrix(new_tweets_df['realResult'].tolist(),new_tweets_df['screenName'].tolist()).tolist()\n",
    "a =['Name','Trump','Kim','Katy','Kent']\n",
    "\n",
    "# add column to matrix\n",
    "for i in range(0,4):\n",
    "    c_m[i].reverse()\n",
    "    c_m[i].append(a[i+1])\n",
    "    c_m[i].reverse()\n",
    "\n",
    "# add row to matrix\n",
    "c_m.reverse()\n",
    "c_m.append(a)\n",
    "c_m.reverse()\n",
    "\n",
    "print('\\n'.join(['\\t'.join(['{:4}'.format(item) for item in row]) for row in c_m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain, each cell representing the number of times that we predicted that the tweet belongs to the celebrity name from the column and the tweet is really belogns to the celebrity name from the row. therefore the diagonal representing the number of times that we predicted the correct result.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
